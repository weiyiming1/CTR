{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./train.csv\"\n",
    "test_file = \"./test.csv\"\n",
    "num_cols = [\"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\"]\n",
    "ignore_cols = [\"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\", \"ps_calc_05\", \n",
    "               \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \n",
    "               \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\"ps_calc_15_bin\", \"ps_calc_16_bin\", \n",
    "               \"ps_calc_17_bin\",\"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"feature_size\": None,\n",
    "    \"field_size\": None,\n",
    "    \"embed_size\":128,\n",
    "    \"deep_nn\":[256,256],\n",
    "    \"dropout_fm\": 0,\n",
    "    \"dropout_deep\": 0.2,\n",
    "    \"epoch\":200,\n",
    "    \"batch\":10000,\n",
    "    \"split\": 0.2,\n",
    "    \"class_weight\": None \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview(cfg):    \n",
    "    dfTrain = pd.read_csv(train_file)\n",
    "    dfTest = pd.read_csv(test_file)\n",
    "    df = pd.concat([dfTrain,dfTest], sort=False)\n",
    "\n",
    "    field_size = len(df.columns) - len(ignore_cols)\n",
    "    feature_dict = {}\n",
    "    feature_size = 0\n",
    "    for col in df.columns:\n",
    "        if col in ignore_cols:\n",
    "            continue\n",
    "        elif col in num_cols:\n",
    "            feature_dict[col] = feature_size\n",
    "            feature_size += 1\n",
    "        else:\n",
    "            unique_val = df[col].unique()\n",
    "            feature_dict[col] = dict(zip(unique_val,range(feature_size,len(unique_val) + feature_size)))\n",
    "            feature_size += len(unique_val)\n",
    "    \n",
    "    cfg['field_size'] = field_size\n",
    "    cfg['feature_size'] = feature_size\n",
    "    return dfTrain, feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain, feature_dict = overview(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_df, cfg):\n",
    "    label_df = train_df[['target']]\n",
    "    neg, pos = np.bincount(label_df.values.flatten())\n",
    "    cfg[\"class_weight\"] = {0: (1/neg)*(neg+pos), 1: (1/pos)*(neg+pos)}\n",
    "    \n",
    "    train_df.drop(['target','id'],axis=1,inplace=True)\n",
    "    feature_idx = train_df.copy()\n",
    "    feature_val = train_df.copy()\n",
    "    for col in feature_idx.columns:\n",
    "        if col in ignore_cols:\n",
    "            feature_idx.drop(col,axis=1,inplace=True)\n",
    "            feature_val.drop(col,axis=1,inplace=True)\n",
    "            continue\n",
    "        elif col in num_cols:\n",
    "            feature_idx[col] = feature_dict[col]\n",
    "        else:\n",
    "            feature_idx[col] = feature_idx[col].map(feature_dict[col])\n",
    "            feature_val[col] = 1        \n",
    "            \n",
    "    split_idx = feature_idx.shape[0] - round(feature_idx.shape[0]*cfg[\"split\"])\n",
    "    train_input = [feature_idx[:split_idx].values, feature_val[:split_idx].values]\n",
    "    train_y = label_df[:split_idx].values\n",
    "    validate_input = [feature_idx[split_idx:].values, feature_val[split_idx:].values]\n",
    "    validate_y = label_df[split_idx:].values\n",
    "\n",
    "    return train_input, train_y, validate_input, validate_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_y, validate_input, validate_y = preprocess(dfTrain, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(tf.keras.Model):\n",
    "    def __init__(self, cfg):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_size = cfg['feature_size']\n",
    "        self.field_size = cfg['field_size']\n",
    "        self.embed_size = cfg['embed_size']\n",
    "        self.deep_nn = cfg['deep_nn']\n",
    "        \n",
    "        self.dropout_fm = cfg['dropout_fm']\n",
    "        self.dropout_deep = cfg['dropout_deep']\n",
    "        \n",
    "        # fm        \n",
    "        self.feature_weight = tf.keras.layers.Embedding(cfg['feature_size'], 1)\n",
    "        self.feature_embed = tf.keras.layers.Embedding(cfg['feature_size'], cfg['embed_size'])\n",
    "\n",
    "        # dnn\n",
    "        for layer in range(len(cfg['deep_nn'])):\n",
    "            setattr(self, 'dense_' + str(layer), tf.keras.layers.Dense(self.deep_nn[layer]))\n",
    "            setattr(self, 'batchNorm_' + str(layer), tf.keras.layers.BatchNormalization())\n",
    "            setattr(self, 'activation_' + str(layer), tf.keras.layers.Activation('relu'))\n",
    "            setattr(self, 'dropout_' + str(layer), tf.keras.layers.Dropout(self.dropout_deep))\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        # inputs = [feature_idx, feature_val]\n",
    "        reshaped_feature_val = tf.cast(tf.reshape(inputs[1], shape=[-1,self.field_size,1]), tf.float32)\n",
    "        # linear        \n",
    "        weights = self.feature_weight(inputs[0])\n",
    "        linear = tf.reduce_sum(tf.multiply(weights,reshaped_feature_val),2)\n",
    "        \n",
    "        # fm  \n",
    "        embeddings = self.feature_embed(inputs[0])\n",
    "        second_inner = tf.multiply(embeddings,reshaped_feature_val)\n",
    "        \n",
    "        summed_features_emb = tf.reduce_sum(second_inner,1)\n",
    "        summed_features_emb_square = tf.square(summed_features_emb)\n",
    "        \n",
    "        squared_features_emb = tf.square(second_inner)\n",
    "        squared_sum_features_emb = tf.reduce_sum(squared_features_emb,1)\n",
    "        \n",
    "        fm = 0.5 * tf.subtract(summed_features_emb_square,squared_sum_features_emb)\n",
    "        \n",
    "        # dnn\n",
    "        y_deep = tf.reshape(embeddings,shape=[-1,self.field_size * self.embed_size])\n",
    "        for layer in range(0, len(self.deep_nn)):\n",
    "            y_deep = getattr(self, 'dense_' + str(layer))(y_deep)\n",
    "            y_deep = getattr(self, 'batchNorm_' + str(layer))(y_deep, training=training)\n",
    "            y_deep = getattr(self, 'activation_' + str(layer))(y_deep)\n",
    "            y_deep = getattr(self, 'dropout_' + str(layer))(y_deep, training=training)\n",
    "            \n",
    "        # concat\n",
    "        concat = tf.concat([linear, fm, y_deep], axis=1)                                \n",
    "        out = self.fc(concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_size': 257,\n",
       " 'field_size': 37,\n",
       " 'embed_size': 128,\n",
       " 'deep_nn': [256, 256],\n",
       " 'dropout_fm': 0,\n",
       " 'dropout_deep': 0.2,\n",
       " 'epoch': 200,\n",
       " 'batch': 10000,\n",
       " 'split': 0.2,\n",
       " 'class_weight': {0: 1.0378261885415976, 1: 27.43671061122891}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='bin_acc'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 476170 samples, validate on 119042 samples\n",
      "Epoch 1/200\n",
      "476170/476170 [==============================] - 8s 17us/sample - loss: 1.3725 - tp: 5159.0000 - fp: 78303.0000 - tn: 380526.0000 - fn: 12182.0000 - bin_acc: 0.8100 - precision: 0.0618 - recall: 0.2975 - auc: 0.5999 - val_loss: 1.3901 - val_tp: 1.0000 - val_fp: 3.0000 - val_tn: 114686.0000 - val_fn: 4352.0000 - val_bin_acc: 0.9634 - val_precision: 0.2500 - val_recall: 2.2973e-04 - val_auc: 0.5542\n",
      "Epoch 2/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3562 - tp: 4079.0000 - fp: 49155.0000 - tn: 409674.0000 - fn: 13262.0000 - bin_acc: 0.8689 - precision: 0.0766 - recall: 0.2352 - auc: 0.6261 - val_loss: 1.3807 - val_tp: 12.0000 - val_fp: 62.0000 - val_tn: 114627.0000 - val_fn: 4341.0000 - val_bin_acc: 0.9630 - val_precision: 0.1622 - val_recall: 0.0028 - val_auc: 0.6167\n",
      "Epoch 3/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3531 - tp: 4395.0000 - fp: 51547.0000 - tn: 407282.0000 - fn: 12946.0000 - bin_acc: 0.8646 - precision: 0.0786 - recall: 0.2534 - auc: 0.6304 - val_loss: 1.3774 - val_tp: 22.0000 - val_fp: 135.0000 - val_tn: 114554.0000 - val_fn: 4331.0000 - val_bin_acc: 0.9625 - val_precision: 0.1401 - val_recall: 0.0051 - val_auc: 0.6227\n",
      "Epoch 4/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3503 - tp: 4565.0000 - fp: 52630.0000 - tn: 406199.0000 - fn: 12776.0000 - bin_acc: 0.8626 - precision: 0.0798 - recall: 0.2632 - auc: 0.6339 - val_loss: 1.3731 - val_tp: 90.0000 - val_fp: 570.0000 - val_tn: 114119.0000 - val_fn: 4263.0000 - val_bin_acc: 0.9594 - val_precision: 0.1364 - val_recall: 0.0207 - val_auc: 0.6238\n",
      "Epoch 5/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3470 - tp: 4869.0000 - fp: 54461.0000 - tn: 404368.0000 - fn: 12472.0000 - bin_acc: 0.8594 - precision: 0.0821 - recall: 0.2808 - auc: 0.6377 - val_loss: 1.3687 - val_tp: 235.0000 - val_fp: 1912.0000 - val_tn: 112777.0000 - val_fn: 4118.0000 - val_bin_acc: 0.9493 - val_precision: 0.1095 - val_recall: 0.0540 - val_auc: 0.6242\n",
      "Epoch 6/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3442 - tp: 5126.0000 - fp: 55697.0000 - tn: 403132.0000 - fn: 12215.0000 - bin_acc: 0.8574 - precision: 0.0843 - recall: 0.2956 - auc: 0.6387 - val_loss: 1.3658 - val_tp: 356.0000 - val_fp: 3453.0000 - val_tn: 111236.0000 - val_fn: 3997.0000 - val_bin_acc: 0.9374 - val_precision: 0.0935 - val_recall: 0.0818 - val_auc: 0.6238\n",
      "Epoch 7/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3414 - tp: 5185.0000 - fp: 54445.0000 - tn: 404384.0000 - fn: 12156.0000 - bin_acc: 0.8601 - precision: 0.0870 - recall: 0.2990 - auc: 0.6430 - val_loss: 1.3614 - val_tp: 679.0000 - val_fp: 7335.0000 - val_tn: 107354.0000 - val_fn: 3674.0000 - val_bin_acc: 0.9075 - val_precision: 0.0847 - val_recall: 0.1560 - val_auc: 0.6231\n",
      "Epoch 8/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3393 - tp: 5460.0000 - fp: 57079.0000 - tn: 401750.0000 - fn: 11881.0000 - bin_acc: 0.8552 - precision: 0.0873 - recall: 0.3149 - auc: 0.6439 - val_loss: 1.3606 - val_tp: 774.0000 - val_fp: 8522.0000 - val_tn: 106167.0000 - val_fn: 3579.0000 - val_bin_acc: 0.8983 - val_precision: 0.0833 - val_recall: 0.1778 - val_auc: 0.6197\n",
      "Epoch 9/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3367 - tp: 5476.0000 - fp: 54532.0000 - tn: 404297.0000 - fn: 11865.0000 - bin_acc: 0.8606 - precision: 0.0913 - recall: 0.3158 - auc: 0.6449 - val_loss: 1.3656 - val_tp: 524.0000 - val_fp: 5812.0000 - val_tn: 108877.0000 - val_fn: 3829.0000 - val_bin_acc: 0.9190 - val_precision: 0.0827 - val_recall: 0.1204 - val_auc: 0.5987\n",
      "Epoch 10/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3339 - tp: 5803.0000 - fp: 57432.0000 - tn: 401397.0000 - fn: 11538.0000 - bin_acc: 0.8552 - precision: 0.0918 - recall: 0.3346 - auc: 0.6480 - val_loss: 1.3626 - val_tp: 803.0000 - val_fp: 9435.0000 - val_tn: 105254.0000 - val_fn: 3550.0000 - val_bin_acc: 0.8909 - val_precision: 0.0784 - val_recall: 0.1845 - val_auc: 0.6060\n",
      "Epoch 11/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3308 - tp: 5851.0000 - fp: 55980.0000 - tn: 402849.0000 - fn: 11490.0000 - bin_acc: 0.8583 - precision: 0.0946 - recall: 0.3374 - auc: 0.6476 - val_loss: 1.3636 - val_tp: 847.0000 - val_fp: 10417.0000 - val_tn: 104272.0000 - val_fn: 3506.0000 - val_bin_acc: 0.8830 - val_precision: 0.0752 - val_recall: 0.1946 - val_auc: 0.5978\n",
      "Epoch 12/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3281 - tp: 6074.0000 - fp: 57119.0000 - tn: 401710.0000 - fn: 11267.0000 - bin_acc: 0.8564 - precision: 0.0961 - recall: 0.3503 - auc: 0.6506 - val_loss: 1.3650 - val_tp: 741.0000 - val_fp: 8547.0000 - val_tn: 106142.0000 - val_fn: 3612.0000 - val_bin_acc: 0.8979 - val_precision: 0.0798 - val_recall: 0.1702 - val_auc: 0.5948\n",
      "Epoch 13/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3254 - tp: 6147.0000 - fp: 56293.0000 - tn: 402536.0000 - fn: 11194.0000 - bin_acc: 0.8583 - precision: 0.0984 - recall: 0.3545 - auc: 0.6551 - val_loss: 1.3663 - val_tp: 1104.0000 - val_fp: 14927.0000 - val_tn: 99762.0000 - val_fn: 3249.0000 - val_bin_acc: 0.8473 - val_precision: 0.0689 - val_recall: 0.2536 - val_auc: 0.6029\n",
      "Epoch 14/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3229 - tp: 6184.0000 - fp: 54912.0000 - tn: 403917.0000 - fn: 11157.0000 - bin_acc: 0.8612 - precision: 0.1012 - recall: 0.3566 - auc: 0.6544 - val_loss: 1.3653 - val_tp: 1165.0000 - val_fp: 15951.0000 - val_tn: 98738.0000 - val_fn: 3188.0000 - val_bin_acc: 0.8392 - val_precision: 0.0681 - val_recall: 0.2676 - val_auc: 0.6020\n",
      "Epoch 15/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3206 - tp: 6267.0000 - fp: 54371.0000 - tn: 404458.0000 - fn: 11074.0000 - bin_acc: 0.8626 - precision: 0.1034 - recall: 0.3614 - auc: 0.6562 - val_loss: 1.3665 - val_tp: 772.0000 - val_fp: 9365.0000 - val_tn: 105324.0000 - val_fn: 3581.0000 - val_bin_acc: 0.8912 - val_precision: 0.0762 - val_recall: 0.1773 - val_auc: 0.5867\n",
      "Epoch 16/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3179 - tp: 6444.0000 - fp: 55177.0000 - tn: 403652.0000 - fn: 10897.0000 - bin_acc: 0.8612 - precision: 0.1046 - recall: 0.3716 - auc: 0.6582 - val_loss: 1.3691 - val_tp: 522.0000 - val_fp: 5672.0000 - val_tn: 109017.0000 - val_fn: 3831.0000 - val_bin_acc: 0.9202 - val_precision: 0.0843 - val_recall: 0.1199 - val_auc: 0.5742\n",
      "Epoch 17/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3158 - tp: 6549.0000 - fp: 54893.0000 - tn: 403936.0000 - fn: 10792.0000 - bin_acc: 0.8621 - precision: 0.1066 - recall: 0.3777 - auc: 0.6611 - val_loss: 1.3676 - val_tp: 673.0000 - val_fp: 8036.0000 - val_tn: 106653.0000 - val_fn: 3680.0000 - val_bin_acc: 0.9016 - val_precision: 0.0773 - val_recall: 0.1546 - val_auc: 0.5798\n",
      "Epoch 18/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3131 - tp: 6656.0000 - fp: 54721.0000 - tn: 404108.0000 - fn: 10685.0000 - bin_acc: 0.8626 - precision: 0.1084 - recall: 0.3838 - auc: 0.6618 - val_loss: 1.3686 - val_tp: 1091.0000 - val_fp: 15009.0000 - val_tn: 99680.0000 - val_fn: 3262.0000 - val_bin_acc: 0.8465 - val_precision: 0.0678 - val_recall: 0.2506 - val_auc: 0.5946\n",
      "Epoch 19/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3115 - tp: 6684.0000 - fp: 53410.0000 - tn: 405419.0000 - fn: 10657.0000 - bin_acc: 0.8655 - precision: 0.1112 - recall: 0.3854 - auc: 0.6639 - val_loss: 1.3712 - val_tp: 1289.0000 - val_fp: 19171.0000 - val_tn: 95518.0000 - val_fn: 3064.0000 - val_bin_acc: 0.8132 - val_precision: 0.0630 - val_recall: 0.2961 - val_auc: 0.5971\n",
      "Epoch 20/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.3086 - tp: 6868.0000 - fp: 54916.0000 - tn: 403913.0000 - fn: 10473.0000 - bin_acc: 0.8627 - precision: 0.1112 - recall: 0.3961 - auc: 0.6674 - val_loss: 1.3664 - val_tp: 697.0000 - val_fp: 8034.0000 - val_tn: 106655.0000 - val_fn: 3656.0000 - val_bin_acc: 0.9018 - val_precision: 0.0798 - val_recall: 0.1601 - val_auc: 0.5760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3068 - tp: 6960.0000 - fp: 54640.0000 - tn: 404189.0000 - fn: 10381.0000 - bin_acc: 0.8635 - precision: 0.1130 - recall: 0.4014 - auc: 0.6689 - val_loss: 1.3677 - val_tp: 640.0000 - val_fp: 7524.0000 - val_tn: 107165.0000 - val_fn: 3713.0000 - val_bin_acc: 0.9056 - val_precision: 0.0784 - val_recall: 0.1470 - val_auc: 0.5770\n",
      "Epoch 22/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3050 - tp: 6988.0000 - fp: 53764.0000 - tn: 405065.0000 - fn: 10353.0000 - bin_acc: 0.8653 - precision: 0.1150 - recall: 0.4030 - auc: 0.6707 - val_loss: 1.3681 - val_tp: 984.0000 - val_fp: 13012.0000 - val_tn: 101677.0000 - val_fn: 3369.0000 - val_bin_acc: 0.8624 - val_precision: 0.0703 - val_recall: 0.2261 - val_auc: 0.5871\n",
      "Epoch 23/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.3034 - tp: 7094.0000 - fp: 54383.0000 - tn: 404446.0000 - fn: 10247.0000 - bin_acc: 0.8643 - precision: 0.1154 - recall: 0.4091 - auc: 0.6739 - val_loss: 1.3701 - val_tp: 1059.0000 - val_fp: 14748.0000 - val_tn: 99941.0000 - val_fn: 3294.0000 - val_bin_acc: 0.8484 - val_precision: 0.0670 - val_recall: 0.2433 - val_auc: 0.5895\n",
      "Epoch 24/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.3013 - tp: 7061.0000 - fp: 52899.0000 - tn: 405930.0000 - fn: 10280.0000 - bin_acc: 0.8673 - precision: 0.1178 - recall: 0.4072 - auc: 0.6747 - val_loss: 1.3732 - val_tp: 1342.0000 - val_fp: 20274.0000 - val_tn: 94415.0000 - val_fn: 3011.0000 - val_bin_acc: 0.8044 - val_precision: 0.0621 - val_recall: 0.3083 - val_auc: 0.5956\n",
      "Epoch 25/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.3004 - tp: 7215.0000 - fp: 53715.0000 - tn: 405114.0000 - fn: 10126.0000 - bin_acc: 0.8659 - precision: 0.1184 - recall: 0.4161 - auc: 0.6751 - val_loss: 1.3687 - val_tp: 837.0000 - val_fp: 10700.0000 - val_tn: 103989.0000 - val_fn: 3516.0000 - val_bin_acc: 0.8806 - val_precision: 0.0725 - val_recall: 0.1923 - val_auc: 0.5806\n",
      "Epoch 26/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2968 - tp: 7188.0000 - fp: 50666.0000 - tn: 408163.0000 - fn: 10153.0000 - bin_acc: 0.8723 - precision: 0.1242 - recall: 0.4145 - auc: 0.6765 - val_loss: 1.3756 - val_tp: 1325.0000 - val_fp: 20442.0000 - val_tn: 94247.0000 - val_fn: 3028.0000 - val_bin_acc: 0.8028 - val_precision: 0.0609 - val_recall: 0.3044 - val_auc: 0.5931\n",
      "Epoch 27/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2948 - tp: 7403.0000 - fp: 52677.0000 - tn: 406152.0000 - fn: 9938.0000 - bin_acc: 0.8685 - precision: 0.1232 - recall: 0.4269 - auc: 0.6801 - val_loss: 1.3690 - val_tp: 965.0000 - val_fp: 13028.0000 - val_tn: 101661.0000 - val_fn: 3388.0000 - val_bin_acc: 0.8621 - val_precision: 0.0690 - val_recall: 0.2217 - val_auc: 0.5856\n",
      "Epoch 28/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2936 - tp: 7344.0000 - fp: 51155.0000 - tn: 407674.0000 - fn: 9997.0000 - bin_acc: 0.8716 - precision: 0.1255 - recall: 0.4235 - auc: 0.6814 - val_loss: 1.3681 - val_tp: 737.0000 - val_fp: 8785.0000 - val_tn: 105904.0000 - val_fn: 3616.0000 - val_bin_acc: 0.8958 - val_precision: 0.0774 - val_recall: 0.1693 - val_auc: 0.5749\n",
      "Epoch 29/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2916 - tp: 7410.0000 - fp: 50023.0000 - tn: 408806.0000 - fn: 9931.0000 - bin_acc: 0.8741 - precision: 0.1290 - recall: 0.4273 - auc: 0.6826 - val_loss: 1.3700 - val_tp: 727.0000 - val_fp: 9307.0000 - val_tn: 105382.0000 - val_fn: 3626.0000 - val_bin_acc: 0.8914 - val_precision: 0.0725 - val_recall: 0.1670 - val_auc: 0.5782\n",
      "Epoch 30/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2906 - tp: 7424.0000 - fp: 49754.0000 - tn: 409075.0000 - fn: 9917.0000 - bin_acc: 0.8747 - precision: 0.1298 - recall: 0.4281 - auc: 0.6833 - val_loss: 1.3741 - val_tp: 1358.0000 - val_fp: 20390.0000 - val_tn: 94299.0000 - val_fn: 2995.0000 - val_bin_acc: 0.8036 - val_precision: 0.0624 - val_recall: 0.3120 - val_auc: 0.5929\n",
      "Epoch 31/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2893 - tp: 7501.0000 - fp: 50295.0000 - tn: 408534.0000 - fn: 9840.0000 - bin_acc: 0.8737 - precision: 0.1298 - recall: 0.4326 - auc: 0.6850 - val_loss: 1.3713 - val_tp: 936.0000 - val_fp: 13014.0000 - val_tn: 101675.0000 - val_fn: 3417.0000 - val_bin_acc: 0.8620 - val_precision: 0.0671 - val_recall: 0.2150 - val_auc: 0.5796\n",
      "Epoch 32/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2887 - tp: 7472.0000 - fp: 49631.0000 - tn: 409198.0000 - fn: 9869.0000 - bin_acc: 0.8750 - precision: 0.1309 - recall: 0.4309 - auc: 0.6866 - val_loss: 1.3690 - val_tp: 1054.0000 - val_fp: 14299.0000 - val_tn: 100390.0000 - val_fn: 3299.0000 - val_bin_acc: 0.8522 - val_precision: 0.0687 - val_recall: 0.2421 - val_auc: 0.5851\n",
      "Epoch 33/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2868 - tp: 7555.0000 - fp: 49529.0000 - tn: 409300.0000 - fn: 9786.0000 - bin_acc: 0.8754 - precision: 0.1323 - recall: 0.4357 - auc: 0.6877 - val_loss: 1.3755 - val_tp: 1323.0000 - val_fp: 20288.0000 - val_tn: 94401.0000 - val_fn: 3030.0000 - val_bin_acc: 0.8041 - val_precision: 0.0612 - val_recall: 0.3039 - val_auc: 0.5899\n",
      "Epoch 34/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2857 - tp: 7574.0000 - fp: 48727.0000 - tn: 410102.0000 - fn: 9767.0000 - bin_acc: 0.8772 - precision: 0.1345 - recall: 0.4368 - auc: 0.6897 - val_loss: 1.3727 - val_tp: 1202.0000 - val_fp: 17644.0000 - val_tn: 97045.0000 - val_fn: 3151.0000 - val_bin_acc: 0.8253 - val_precision: 0.0638 - val_recall: 0.2761 - val_auc: 0.5895\n",
      "Epoch 35/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2846 - tp: 7688.0000 - fp: 49683.0000 - tn: 409146.0000 - fn: 9653.0000 - bin_acc: 0.8754 - precision: 0.1340 - recall: 0.4433 - auc: 0.6913 - val_loss: 1.3748 - val_tp: 1173.0000 - val_fp: 17608.0000 - val_tn: 97081.0000 - val_fn: 3180.0000 - val_bin_acc: 0.8254 - val_precision: 0.0625 - val_recall: 0.2695 - val_auc: 0.5840\n",
      "Epoch 36/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2826 - tp: 7688.0000 - fp: 48367.0000 - tn: 410462.0000 - fn: 9653.0000 - bin_acc: 0.8782 - precision: 0.1372 - recall: 0.4433 - auc: 0.6923 - val_loss: 1.3722 - val_tp: 1142.0000 - val_fp: 16470.0000 - val_tn: 98219.0000 - val_fn: 3211.0000 - val_bin_acc: 0.8347 - val_precision: 0.0648 - val_recall: 0.2623 - val_auc: 0.5863\n",
      "Epoch 37/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2819 - tp: 7776.0000 - fp: 48731.0000 - tn: 410098.0000 - fn: 9565.0000 - bin_acc: 0.8776 - precision: 0.1376 - recall: 0.4484 - auc: 0.6929 - val_loss: 1.3734 - val_tp: 1230.0000 - val_fp: 18172.0000 - val_tn: 96517.0000 - val_fn: 3123.0000 - val_bin_acc: 0.8211 - val_precision: 0.0634 - val_recall: 0.2826 - val_auc: 0.5880\n",
      "Epoch 38/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2786 - tp: 7824.0000 - fp: 47210.0000 - tn: 411619.0000 - fn: 9517.0000 - bin_acc: 0.8809 - precision: 0.1422 - recall: 0.4512 - auc: 0.6964 - val_loss: 1.3704 - val_tp: 986.0000 - val_fp: 13428.0000 - val_tn: 101261.0000 - val_fn: 3367.0000 - val_bin_acc: 0.8589 - val_precision: 0.0684 - val_recall: 0.2265 - val_auc: 0.5808\n",
      "Epoch 39/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2794 - tp: 7788.0000 - fp: 48026.0000 - tn: 410803.0000 - fn: 9553.0000 - bin_acc: 0.8791 - precision: 0.1395 - recall: 0.4491 - auc: 0.6969 - val_loss: 1.3716 - val_tp: 884.0000 - val_fp: 12248.0000 - val_tn: 102441.0000 - val_fn: 3469.0000 - val_bin_acc: 0.8680 - val_precision: 0.0673 - val_recall: 0.2031 - val_auc: 0.5779\n",
      "Epoch 40/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2776 - tp: 7824.0000 - fp: 47153.0000 - tn: 411676.0000 - fn: 9517.0000 - bin_acc: 0.8810 - precision: 0.1423 - recall: 0.4512 - auc: 0.6980 - val_loss: 1.3814 - val_tp: 1399.0000 - val_fp: 22607.0000 - val_tn: 92082.0000 - val_fn: 2954.0000 - val_bin_acc: 0.7853 - val_precision: 0.0583 - val_recall: 0.3214 - val_auc: 0.5889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2768 - tp: 7909.0000 - fp: 47593.0000 - tn: 411236.0000 - fn: 9432.0000 - bin_acc: 0.8802 - precision: 0.1425 - recall: 0.4561 - auc: 0.6993 - val_loss: 1.3722 - val_tp: 972.0000 - val_fp: 13469.0000 - val_tn: 101220.0000 - val_fn: 3381.0000 - val_bin_acc: 0.8585 - val_precision: 0.0673 - val_recall: 0.2233 - val_auc: 0.5787\n",
      "Epoch 42/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2750 - tp: 7937.0000 - fp: 46442.0000 - tn: 412387.0000 - fn: 9404.0000 - bin_acc: 0.8827 - precision: 0.1460 - recall: 0.4577 - auc: 0.7000 - val_loss: 1.3744 - val_tp: 1140.0000 - val_fp: 16740.0000 - val_tn: 97949.0000 - val_fn: 3213.0000 - val_bin_acc: 0.8324 - val_precision: 0.0638 - val_recall: 0.2619 - val_auc: 0.5803\n",
      "Epoch 43/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2735 - tp: 8010.0000 - fp: 46523.0000 - tn: 412306.0000 - fn: 9331.0000 - bin_acc: 0.8827 - precision: 0.1469 - recall: 0.4619 - auc: 0.7021 - val_loss: 1.3704 - val_tp: 766.0000 - val_fp: 9864.0000 - val_tn: 104825.0000 - val_fn: 3587.0000 - val_bin_acc: 0.8870 - val_precision: 0.0721 - val_recall: 0.1760 - val_auc: 0.5718\n",
      "Epoch 44/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2734 - tp: 8048.0000 - fp: 47177.0000 - tn: 411652.0000 - fn: 9293.0000 - bin_acc: 0.8814 - precision: 0.1457 - recall: 0.4641 - auc: 0.7031 - val_loss: 1.3709 - val_tp: 752.0000 - val_fp: 9678.0000 - val_tn: 105011.0000 - val_fn: 3601.0000 - val_bin_acc: 0.8885 - val_precision: 0.0721 - val_recall: 0.1728 - val_auc: 0.5688\n",
      "Epoch 45/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2720 - tp: 8089.0000 - fp: 46878.0000 - tn: 411951.0000 - fn: 9252.0000 - bin_acc: 0.8821 - precision: 0.1472 - recall: 0.4665 - auc: 0.7042 - val_loss: 1.3709 - val_tp: 871.0000 - val_fp: 11792.0000 - val_tn: 102897.0000 - val_fn: 3482.0000 - val_bin_acc: 0.8717 - val_precision: 0.0688 - val_recall: 0.2001 - val_auc: 0.5779\n",
      "Epoch 46/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2713 - tp: 8096.0000 - fp: 45986.0000 - tn: 412843.0000 - fn: 9245.0000 - bin_acc: 0.8840 - precision: 0.1497 - recall: 0.4669 - auc: 0.7048 - val_loss: 1.3693 - val_tp: 675.0000 - val_fp: 8242.0000 - val_tn: 106447.0000 - val_fn: 3678.0000 - val_bin_acc: 0.8999 - val_precision: 0.0757 - val_recall: 0.1551 - val_auc: 0.5697\n",
      "Epoch 47/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2710 - tp: 8123.0000 - fp: 46887.0000 - tn: 411942.0000 - fn: 9218.0000 - bin_acc: 0.8822 - precision: 0.1477 - recall: 0.4684 - auc: 0.7053 - val_loss: 1.3775 - val_tp: 1145.0000 - val_fp: 17752.0000 - val_tn: 96937.0000 - val_fn: 3208.0000 - val_bin_acc: 0.8239 - val_precision: 0.0606 - val_recall: 0.2630 - val_auc: 0.5797\n",
      "Epoch 48/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2689 - tp: 8228.0000 - fp: 46487.0000 - tn: 412342.0000 - fn: 9113.0000 - bin_acc: 0.8832 - precision: 0.1504 - recall: 0.4745 - auc: 0.7064 - val_loss: 1.3732 - val_tp: 800.0000 - val_fp: 10987.0000 - val_tn: 103702.0000 - val_fn: 3553.0000 - val_bin_acc: 0.8779 - val_precision: 0.0679 - val_recall: 0.1838 - val_auc: 0.5680\n",
      "Epoch 49/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2686 - tp: 8176.0000 - fp: 45366.0000 - tn: 413463.0000 - fn: 9165.0000 - bin_acc: 0.8855 - precision: 0.1527 - recall: 0.4715 - auc: 0.7071 - val_loss: 1.3700 - val_tp: 776.0000 - val_fp: 9781.0000 - val_tn: 104908.0000 - val_fn: 3577.0000 - val_bin_acc: 0.8878 - val_precision: 0.0735 - val_recall: 0.1783 - val_auc: 0.5699\n",
      "Epoch 50/200\n",
      "476170/476170 [==============================] - 6s 14us/sample - loss: 1.2668 - tp: 8255.0000 - fp: 45344.0000 - tn: 413485.0000 - fn: 9086.0000 - bin_acc: 0.8857 - precision: 0.1540 - recall: 0.4760 - auc: 0.7091 - val_loss: 1.3730 - val_tp: 919.0000 - val_fp: 12870.0000 - val_tn: 101819.0000 - val_fn: 3434.0000 - val_bin_acc: 0.8630 - val_precision: 0.0666 - val_recall: 0.2111 - val_auc: 0.5741\n",
      "Epoch 51/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2677 - tp: 8143.0000 - fp: 44641.0000 - tn: 414188.0000 - fn: 9198.0000 - bin_acc: 0.8869 - precision: 0.1543 - recall: 0.4696 - auc: 0.7087 - val_loss: 1.3737 - val_tp: 972.0000 - val_fp: 13985.0000 - val_tn: 100704.0000 - val_fn: 3381.0000 - val_bin_acc: 0.8541 - val_precision: 0.0650 - val_recall: 0.2233 - val_auc: 0.5748\n",
      "Epoch 52/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2659 - tp: 8261.0000 - fp: 45275.0000 - tn: 413554.0000 - fn: 9080.0000 - bin_acc: 0.8858 - precision: 0.1543 - recall: 0.4764 - auc: 0.7097 - val_loss: 1.3734 - val_tp: 867.0000 - val_fp: 11879.0000 - val_tn: 102810.0000 - val_fn: 3486.0000 - val_bin_acc: 0.8709 - val_precision: 0.0680 - val_recall: 0.1992 - val_auc: 0.5678\n",
      "Epoch 53/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2646 - tp: 8290.0000 - fp: 44689.0000 - tn: 414140.0000 - fn: 9051.0000 - bin_acc: 0.8871 - precision: 0.1565 - recall: 0.4781 - auc: 0.7126 - val_loss: 1.3750 - val_tp: 1056.0000 - val_fp: 15567.0000 - val_tn: 99122.0000 - val_fn: 3297.0000 - val_bin_acc: 0.8415 - val_precision: 0.0635 - val_recall: 0.2426 - val_auc: 0.5768\n",
      "Epoch 54/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2657 - tp: 8337.0000 - fp: 46271.0000 - tn: 412558.0000 - fn: 9004.0000 - bin_acc: 0.8839 - precision: 0.1527 - recall: 0.4808 - auc: 0.7123 - val_loss: 1.3709 - val_tp: 713.0000 - val_fp: 8855.0000 - val_tn: 105834.0000 - val_fn: 3640.0000 - val_bin_acc: 0.8950 - val_precision: 0.0745 - val_recall: 0.1638 - val_auc: 0.5656\n",
      "Epoch 55/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2644 - tp: 8318.0000 - fp: 44783.0000 - tn: 414046.0000 - fn: 9023.0000 - bin_acc: 0.8870 - precision: 0.1566 - recall: 0.4797 - auc: 0.7129 - val_loss: 1.3736 - val_tp: 995.0000 - val_fp: 14339.0000 - val_tn: 100350.0000 - val_fn: 3358.0000 - val_bin_acc: 0.8513 - val_precision: 0.0649 - val_recall: 0.2286 - val_auc: 0.5753\n",
      "Epoch 56/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2620 - tp: 8323.0000 - fp: 43044.0000 - tn: 415785.0000 - fn: 9018.0000 - bin_acc: 0.8907 - precision: 0.1620 - recall: 0.4800 - auc: 0.7144 - val_loss: 1.3730 - val_tp: 829.0000 - val_fp: 11376.0000 - val_tn: 103313.0000 - val_fn: 3524.0000 - val_bin_acc: 0.8748 - val_precision: 0.0679 - val_recall: 0.1904 - val_auc: 0.5716\n",
      "Epoch 57/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2624 - tp: 8349.0000 - fp: 43999.0000 - tn: 414830.0000 - fn: 8992.0000 - bin_acc: 0.8887 - precision: 0.1595 - recall: 0.4815 - auc: 0.7144 - val_loss: 1.3713 - val_tp: 787.0000 - val_fp: 10323.0000 - val_tn: 104366.0000 - val_fn: 3566.0000 - val_bin_acc: 0.8833 - val_precision: 0.0708 - val_recall: 0.1808 - val_auc: 0.5665\n",
      "Epoch 58/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2627 - tp: 8349.0000 - fp: 44021.0000 - tn: 414808.0000 - fn: 8992.0000 - bin_acc: 0.8887 - precision: 0.1594 - recall: 0.4815 - auc: 0.7140 - val_loss: 1.3792 - val_tp: 1208.0000 - val_fp: 18941.0000 - val_tn: 95748.0000 - val_fn: 3145.0000 - val_bin_acc: 0.8145 - val_precision: 0.0600 - val_recall: 0.2775 - val_auc: 0.5811\n",
      "Epoch 59/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2590 - tp: 8410.0000 - fp: 41995.0000 - tn: 416834.0000 - fn: 8931.0000 - bin_acc: 0.8931 - precision: 0.1668 - recall: 0.4850 - auc: 0.7176 - val_loss: 1.3792 - val_tp: 1233.0000 - val_fp: 19160.0000 - val_tn: 95529.0000 - val_fn: 3120.0000 - val_bin_acc: 0.8128 - val_precision: 0.0605 - val_recall: 0.2833 - val_auc: 0.5808\n",
      "Epoch 60/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2596 - tp: 8485.0000 - fp: 44246.0000 - tn: 414583.0000 - fn: 8856.0000 - bin_acc: 0.8885 - precision: 0.1609 - recall: 0.4893 - auc: 0.7180 - val_loss: 1.3720 - val_tp: 899.0000 - val_fp: 12325.0000 - val_tn: 102364.0000 - val_fn: 3454.0000 - val_bin_acc: 0.8675 - val_precision: 0.0680 - val_recall: 0.2065 - val_auc: 0.5689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2600 - tp: 8457.0000 - fp: 44128.0000 - tn: 414701.0000 - fn: 8884.0000 - bin_acc: 0.8887 - precision: 0.1608 - recall: 0.4877 - auc: 0.7179 - val_loss: 1.3726 - val_tp: 823.0000 - val_fp: 11172.0000 - val_tn: 103517.0000 - val_fn: 3530.0000 - val_bin_acc: 0.8765 - val_precision: 0.0686 - val_recall: 0.1891 - val_auc: 0.5681\n",
      "Epoch 62/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2580 - tp: 8495.0000 - fp: 42824.0000 - tn: 416005.0000 - fn: 8846.0000 - bin_acc: 0.8915 - precision: 0.1655 - recall: 0.4899 - auc: 0.7190 - val_loss: 1.3814 - val_tp: 1176.0000 - val_fp: 18598.0000 - val_tn: 96091.0000 - val_fn: 3177.0000 - val_bin_acc: 0.8171 - val_precision: 0.0595 - val_recall: 0.2702 - val_auc: 0.5791\n",
      "Epoch 63/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2577 - tp: 8457.0000 - fp: 42361.0000 - tn: 416468.0000 - fn: 8884.0000 - bin_acc: 0.8924 - precision: 0.1664 - recall: 0.4877 - auc: 0.7193 - val_loss: 1.3743 - val_tp: 819.0000 - val_fp: 11521.0000 - val_tn: 103168.0000 - val_fn: 3534.0000 - val_bin_acc: 0.8735 - val_precision: 0.0664 - val_recall: 0.1881 - val_auc: 0.5679\n",
      "Epoch 64/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2567 - tp: 8546.0000 - fp: 42505.0000 - tn: 416324.0000 - fn: 8795.0000 - bin_acc: 0.8923 - precision: 0.1674 - recall: 0.4928 - auc: 0.7200 - val_loss: 1.3738 - val_tp: 826.0000 - val_fp: 11321.0000 - val_tn: 103368.0000 - val_fn: 3527.0000 - val_bin_acc: 0.8753 - val_precision: 0.0680 - val_recall: 0.1898 - val_auc: 0.5675\n",
      "Epoch 65/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2565 - tp: 8505.0000 - fp: 42310.0000 - tn: 416519.0000 - fn: 8836.0000 - bin_acc: 0.8926 - precision: 0.1674 - recall: 0.4905 - auc: 0.7208 - val_loss: 1.3733 - val_tp: 823.0000 - val_fp: 11383.0000 - val_tn: 103306.0000 - val_fn: 3530.0000 - val_bin_acc: 0.8747 - val_precision: 0.0674 - val_recall: 0.1891 - val_auc: 0.5679\n",
      "Epoch 66/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2557 - tp: 8542.0000 - fp: 42161.0000 - tn: 416668.0000 - fn: 8799.0000 - bin_acc: 0.8930 - precision: 0.1685 - recall: 0.4926 - auc: 0.7204 - val_loss: 1.3741 - val_tp: 870.0000 - val_fp: 12242.0000 - val_tn: 102447.0000 - val_fn: 3483.0000 - val_bin_acc: 0.8679 - val_precision: 0.0664 - val_recall: 0.1999 - val_auc: 0.5696\n",
      "Epoch 67/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2548 - tp: 8571.0000 - fp: 41874.0000 - tn: 416955.0000 - fn: 8770.0000 - bin_acc: 0.8936 - precision: 0.1699 - recall: 0.4943 - auc: 0.7233 - val_loss: 1.3744 - val_tp: 874.0000 - val_fp: 12442.0000 - val_tn: 102247.0000 - val_fn: 3479.0000 - val_bin_acc: 0.8663 - val_precision: 0.0656 - val_recall: 0.2008 - val_auc: 0.5689\n",
      "Epoch 68/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2542 - tp: 8625.0000 - fp: 41968.0000 - tn: 416861.0000 - fn: 8716.0000 - bin_acc: 0.8936 - precision: 0.1705 - recall: 0.4974 - auc: 0.7228 - val_loss: 1.3740 - val_tp: 953.0000 - val_fp: 13405.0000 - val_tn: 101284.0000 - val_fn: 3400.0000 - val_bin_acc: 0.8588 - val_precision: 0.0664 - val_recall: 0.2189 - val_auc: 0.5722\n",
      "Epoch 69/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2538 - tp: 8648.0000 - fp: 41984.0000 - tn: 416845.0000 - fn: 8693.0000 - bin_acc: 0.8936 - precision: 0.1708 - recall: 0.4987 - auc: 0.7231 - val_loss: 1.3745 - val_tp: 850.0000 - val_fp: 12003.0000 - val_tn: 102686.0000 - val_fn: 3503.0000 - val_bin_acc: 0.8697 - val_precision: 0.0661 - val_recall: 0.1953 - val_auc: 0.5667\n",
      "Epoch 70/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2544 - tp: 8589.0000 - fp: 42227.0000 - tn: 416602.0000 - fn: 8752.0000 - bin_acc: 0.8929 - precision: 0.1690 - recall: 0.4953 - auc: 0.7229 - val_loss: 1.3734 - val_tp: 722.0000 - val_fp: 9584.0000 - val_tn: 105105.0000 - val_fn: 3631.0000 - val_bin_acc: 0.8890 - val_precision: 0.0701 - val_recall: 0.1659 - val_auc: 0.5637\n",
      "Epoch 71/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2543 - tp: 8599.0000 - fp: 42136.0000 - tn: 416693.0000 - fn: 8742.0000 - bin_acc: 0.8932 - precision: 0.1695 - recall: 0.4959 - auc: 0.7236 - val_loss: 1.3745 - val_tp: 973.0000 - val_fp: 13959.0000 - val_tn: 100730.0000 - val_fn: 3380.0000 - val_bin_acc: 0.8543 - val_precision: 0.0652 - val_recall: 0.2235 - val_auc: 0.5730\n",
      "Epoch 72/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2525 - tp: 8684.0000 - fp: 42040.0000 - tn: 416789.0000 - fn: 8657.0000 - bin_acc: 0.8935 - precision: 0.1712 - recall: 0.5008 - auc: 0.7248 - val_loss: 1.3756 - val_tp: 988.0000 - val_fp: 14349.0000 - val_tn: 100340.0000 - val_fn: 3365.0000 - val_bin_acc: 0.8512 - val_precision: 0.0644 - val_recall: 0.2270 - val_auc: 0.5720\n",
      "Epoch 73/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2515 - tp: 8708.0000 - fp: 41801.0000 - tn: 417028.0000 - fn: 8633.0000 - bin_acc: 0.8941 - precision: 0.1724 - recall: 0.5022 - auc: 0.7260 - val_loss: 1.3733 - val_tp: 700.0000 - val_fp: 9036.0000 - val_tn: 105653.0000 - val_fn: 3653.0000 - val_bin_acc: 0.8934 - val_precision: 0.0719 - val_recall: 0.1608 - val_auc: 0.5634\n",
      "Epoch 74/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2513 - tp: 8687.0000 - fp: 40808.0000 - tn: 418021.0000 - fn: 8654.0000 - bin_acc: 0.8961 - precision: 0.1755 - recall: 0.5010 - auc: 0.7256 - val_loss: 1.3749 - val_tp: 938.0000 - val_fp: 13274.0000 - val_tn: 101415.0000 - val_fn: 3415.0000 - val_bin_acc: 0.8598 - val_precision: 0.0660 - val_recall: 0.2155 - val_auc: 0.5696\n",
      "Epoch 75/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2492 - tp: 8766.0000 - fp: 40598.0000 - tn: 418231.0000 - fn: 8575.0000 - bin_acc: 0.8967 - precision: 0.1776 - recall: 0.5055 - auc: 0.7282 - val_loss: 1.3776 - val_tp: 1006.0000 - val_fp: 15082.0000 - val_tn: 99607.0000 - val_fn: 3347.0000 - val_bin_acc: 0.8452 - val_precision: 0.0625 - val_recall: 0.2311 - val_auc: 0.5697\n",
      "Epoch 76/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2504 - tp: 8676.0000 - fp: 40420.0000 - tn: 418409.0000 - fn: 8665.0000 - bin_acc: 0.8969 - precision: 0.1767 - recall: 0.5003 - auc: 0.7272 - val_loss: 1.3738 - val_tp: 730.0000 - val_fp: 9677.0000 - val_tn: 105012.0000 - val_fn: 3623.0000 - val_bin_acc: 0.8883 - val_precision: 0.0701 - val_recall: 0.1677 - val_auc: 0.5609\n",
      "Epoch 77/200\n",
      "476170/476170 [==============================] - 6s 14us/sample - loss: 1.2496 - tp: 8696.0000 - fp: 39609.0000 - tn: 419220.0000 - fn: 8645.0000 - bin_acc: 0.8987 - precision: 0.1800 - recall: 0.5015 - auc: 0.7276 - val_loss: 1.3734 - val_tp: 704.0000 - val_fp: 9239.0000 - val_tn: 105450.0000 - val_fn: 3649.0000 - val_bin_acc: 0.8917 - val_precision: 0.0708 - val_recall: 0.1617 - val_auc: 0.5607\n",
      "Epoch 78/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2500 - tp: 8758.0000 - fp: 41439.0000 - tn: 417390.0000 - fn: 8583.0000 - bin_acc: 0.8949 - precision: 0.1745 - recall: 0.5050 - auc: 0.7287 - val_loss: 1.3777 - val_tp: 885.0000 - val_fp: 13539.0000 - val_tn: 101150.0000 - val_fn: 3468.0000 - val_bin_acc: 0.8571 - val_precision: 0.0614 - val_recall: 0.2033 - val_auc: 0.5685\n",
      "Epoch 79/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2486 - tp: 8759.0000 - fp: 40652.0000 - tn: 418177.0000 - fn: 8582.0000 - bin_acc: 0.8966 - precision: 0.1773 - recall: 0.5051 - auc: 0.7286 - val_loss: 1.3731 - val_tp: 692.0000 - val_fp: 8875.0000 - val_tn: 105814.0000 - val_fn: 3661.0000 - val_bin_acc: 0.8947 - val_precision: 0.0723 - val_recall: 0.1590 - val_auc: 0.5601\n",
      "Epoch 80/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2481 - tp: 8779.0000 - fp: 40611.0000 - tn: 418218.0000 - fn: 8562.0000 - bin_acc: 0.8967 - precision: 0.1777 - recall: 0.5063 - auc: 0.7292 - val_loss: 1.3769 - val_tp: 997.0000 - val_fp: 14677.0000 - val_tn: 100012.0000 - val_fn: 3356.0000 - val_bin_acc: 0.8485 - val_precision: 0.0636 - val_recall: 0.2290 - val_auc: 0.5699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2459 - tp: 8889.0000 - fp: 39688.0000 - tn: 419141.0000 - fn: 8452.0000 - bin_acc: 0.8989 - precision: 0.1830 - recall: 0.5126 - auc: 0.7309 - val_loss: 1.3795 - val_tp: 1055.0000 - val_fp: 16353.0000 - val_tn: 98336.0000 - val_fn: 3298.0000 - val_bin_acc: 0.8349 - val_precision: 0.0606 - val_recall: 0.2424 - val_auc: 0.5713\n",
      "Epoch 82/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2469 - tp: 8801.0000 - fp: 39763.0000 - tn: 419066.0000 - fn: 8540.0000 - bin_acc: 0.8986 - precision: 0.1812 - recall: 0.5075 - auc: 0.7305 - val_loss: 1.3792 - val_tp: 1064.0000 - val_fp: 16279.0000 - val_tn: 98410.0000 - val_fn: 3289.0000 - val_bin_acc: 0.8356 - val_precision: 0.0614 - val_recall: 0.2444 - val_auc: 0.5703\n",
      "Epoch 83/200\n",
      "476170/476170 [==============================] - 5s 12us/sample - loss: 1.2464 - tp: 8824.0000 - fp: 39619.0000 - tn: 419210.0000 - fn: 8517.0000 - bin_acc: 0.8989 - precision: 0.1822 - recall: 0.5089 - auc: 0.7306 - val_loss: 1.3766 - val_tp: 964.0000 - val_fp: 14179.0000 - val_tn: 100510.0000 - val_fn: 3389.0000 - val_bin_acc: 0.8524 - val_precision: 0.0637 - val_recall: 0.2215 - val_auc: 0.5662\n",
      "Epoch 84/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2449 - tp: 8901.0000 - fp: 39707.0000 - tn: 419122.0000 - fn: 8440.0000 - bin_acc: 0.8989 - precision: 0.1831 - recall: 0.5133 - auc: 0.7328 - val_loss: 1.3759 - val_tp: 806.0000 - val_fp: 11654.0000 - val_tn: 103035.0000 - val_fn: 3547.0000 - val_bin_acc: 0.8723 - val_precision: 0.0647 - val_recall: 0.1852 - val_auc: 0.5650\n",
      "Epoch 85/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2454 - tp: 8848.0000 - fp: 39208.0000 - tn: 419621.0000 - fn: 8493.0000 - bin_acc: 0.8998 - precision: 0.1841 - recall: 0.5102 - auc: 0.7325 - val_loss: 1.3784 - val_tp: 1034.0000 - val_fp: 15609.0000 - val_tn: 99080.0000 - val_fn: 3319.0000 - val_bin_acc: 0.8410 - val_precision: 0.0621 - val_recall: 0.2375 - val_auc: 0.5715\n",
      "Epoch 86/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2464 - tp: 8869.0000 - fp: 40477.0000 - tn: 418352.0000 - fn: 8472.0000 - bin_acc: 0.8972 - precision: 0.1797 - recall: 0.5114 - auc: 0.7325 - val_loss: 1.3748 - val_tp: 769.0000 - val_fp: 10604.0000 - val_tn: 104085.0000 - val_fn: 3584.0000 - val_bin_acc: 0.8808 - val_precision: 0.0676 - val_recall: 0.1767 - val_auc: 0.5634\n",
      "Epoch 87/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2445 - tp: 8912.0000 - fp: 39369.0000 - tn: 419460.0000 - fn: 8429.0000 - bin_acc: 0.8996 - precision: 0.1846 - recall: 0.5139 - auc: 0.7342 - val_loss: 1.3739 - val_tp: 831.0000 - val_fp: 11399.0000 - val_tn: 103290.0000 - val_fn: 3522.0000 - val_bin_acc: 0.8747 - val_precision: 0.0679 - val_recall: 0.1909 - val_auc: 0.5685\n",
      "Epoch 88/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2449 - tp: 8853.0000 - fp: 39319.0000 - tn: 419510.0000 - fn: 8488.0000 - bin_acc: 0.8996 - precision: 0.1838 - recall: 0.5105 - auc: 0.7338 - val_loss: 1.3766 - val_tp: 822.0000 - val_fp: 11881.0000 - val_tn: 102808.0000 - val_fn: 3531.0000 - val_bin_acc: 0.8705 - val_precision: 0.0647 - val_recall: 0.1888 - val_auc: 0.5669\n",
      "Epoch 89/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2429 - tp: 8929.0000 - fp: 38750.0000 - tn: 420079.0000 - fn: 8412.0000 - bin_acc: 0.9010 - precision: 0.1873 - recall: 0.5149 - auc: 0.7347 - val_loss: 1.3748 - val_tp: 708.0000 - val_fp: 9445.0000 - val_tn: 105244.0000 - val_fn: 3645.0000 - val_bin_acc: 0.8900 - val_precision: 0.0697 - val_recall: 0.1626 - val_auc: 0.5578\n",
      "Epoch 90/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2430 - tp: 8924.0000 - fp: 38725.0000 - tn: 420104.0000 - fn: 8417.0000 - bin_acc: 0.9010 - precision: 0.1873 - recall: 0.5146 - auc: 0.7357 - val_loss: 1.3760 - val_tp: 718.0000 - val_fp: 10233.0000 - val_tn: 104456.0000 - val_fn: 3635.0000 - val_bin_acc: 0.8835 - val_precision: 0.0656 - val_recall: 0.1649 - val_auc: 0.5611\n",
      "Epoch 91/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2426 - tp: 8939.0000 - fp: 38685.0000 - tn: 420144.0000 - fn: 8402.0000 - bin_acc: 0.9011 - precision: 0.1877 - recall: 0.5155 - auc: 0.7351 - val_loss: 1.3741 - val_tp: 620.0000 - val_fp: 8199.0000 - val_tn: 106490.0000 - val_fn: 3733.0000 - val_bin_acc: 0.8998 - val_precision: 0.0703 - val_recall: 0.1424 - val_auc: 0.5580\n",
      "Epoch 92/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2416 - tp: 8988.0000 - fp: 38937.0000 - tn: 419892.0000 - fn: 8353.0000 - bin_acc: 0.9007 - precision: 0.1875 - recall: 0.5183 - auc: 0.7376 - val_loss: 1.3798 - val_tp: 1023.0000 - val_fp: 15984.0000 - val_tn: 98705.0000 - val_fn: 3330.0000 - val_bin_acc: 0.8378 - val_precision: 0.0602 - val_recall: 0.2350 - val_auc: 0.5716\n",
      "Epoch 93/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2429 - tp: 8971.0000 - fp: 39439.0000 - tn: 419390.0000 - fn: 8370.0000 - bin_acc: 0.8996 - precision: 0.1853 - recall: 0.5173 - auc: 0.7359 - val_loss: 1.3775 - val_tp: 935.0000 - val_fp: 13842.0000 - val_tn: 100847.0000 - val_fn: 3418.0000 - val_bin_acc: 0.8550 - val_precision: 0.0633 - val_recall: 0.2148 - val_auc: 0.5680\n",
      "Epoch 94/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2411 - tp: 9027.0000 - fp: 39134.0000 - tn: 419695.0000 - fn: 8314.0000 - bin_acc: 0.9004 - precision: 0.1874 - recall: 0.5206 - auc: 0.7373 - val_loss: 1.3774 - val_tp: 840.0000 - val_fp: 12297.0000 - val_tn: 102392.0000 - val_fn: 3513.0000 - val_bin_acc: 0.8672 - val_precision: 0.0639 - val_recall: 0.1930 - val_auc: 0.5615\n",
      "Epoch 95/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2410 - tp: 9029.0000 - fp: 39085.0000 - tn: 419744.0000 - fn: 8312.0000 - bin_acc: 0.9005 - precision: 0.1877 - recall: 0.5207 - auc: 0.7377 - val_loss: 1.3782 - val_tp: 935.0000 - val_fp: 14268.0000 - val_tn: 100421.0000 - val_fn: 3418.0000 - val_bin_acc: 0.8514 - val_precision: 0.0615 - val_recall: 0.2148 - val_auc: 0.5663\n",
      "Epoch 96/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2413 - tp: 9012.0000 - fp: 39055.0000 - tn: 419774.0000 - fn: 8329.0000 - bin_acc: 0.9005 - precision: 0.1875 - recall: 0.5197 - auc: 0.7382 - val_loss: 1.3810 - val_tp: 1061.0000 - val_fp: 16811.0000 - val_tn: 97878.0000 - val_fn: 3292.0000 - val_bin_acc: 0.8311 - val_precision: 0.0594 - val_recall: 0.2437 - val_auc: 0.5692\n",
      "Epoch 97/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2404 - tp: 8974.0000 - fp: 37833.0000 - tn: 420996.0000 - fn: 8367.0000 - bin_acc: 0.9030 - precision: 0.1917 - recall: 0.5175 - auc: 0.7384 - val_loss: 1.3800 - val_tp: 1068.0000 - val_fp: 16290.0000 - val_tn: 98399.0000 - val_fn: 3285.0000 - val_bin_acc: 0.8356 - val_precision: 0.0615 - val_recall: 0.2453 - val_auc: 0.5691\n",
      "Epoch 98/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2392 - tp: 9020.0000 - fp: 37592.0000 - tn: 421237.0000 - fn: 8321.0000 - bin_acc: 0.9036 - precision: 0.1935 - recall: 0.5202 - auc: 0.7402 - val_loss: 1.3768 - val_tp: 825.0000 - val_fp: 12007.0000 - val_tn: 102682.0000 - val_fn: 3528.0000 - val_bin_acc: 0.8695 - val_precision: 0.0643 - val_recall: 0.1895 - val_auc: 0.5636\n",
      "Epoch 99/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2401 - tp: 8985.0000 - fp: 37701.0000 - tn: 421128.0000 - fn: 8356.0000 - bin_acc: 0.9033 - precision: 0.1925 - recall: 0.5181 - auc: 0.7390 - val_loss: 1.3744 - val_tp: 788.0000 - val_fp: 10665.0000 - val_tn: 104024.0000 - val_fn: 3565.0000 - val_bin_acc: 0.8805 - val_precision: 0.0688 - val_recall: 0.1810 - val_auc: 0.5614\n",
      "Epoch 100/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2382 - tp: 9103.0000 - fp: 38015.0000 - tn: 420814.0000 - fn: 8238.0000 - bin_acc: 0.9029 - precision: 0.1932 - recall: 0.5249 - auc: 0.7403 - val_loss: 1.3766 - val_tp: 869.0000 - val_fp: 12695.0000 - val_tn: 101994.0000 - val_fn: 3484.0000 - val_bin_acc: 0.8641 - val_precision: 0.0641 - val_recall: 0.1996 - val_auc: 0.5651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "476170/476170 [==============================] - 5s 11us/sample - loss: 1.2390 - tp: 9041.0000 - fp: 37927.0000 - tn: 420902.0000 - fn: 8300.0000 - bin_acc: 0.9029 - precision: 0.1925 - recall: 0.5214 - auc: 0.7395 - val_loss: 1.3755 - val_tp: 792.0000 - val_fp: 11262.0000 - val_tn: 103427.0000 - val_fn: 3561.0000 - val_bin_acc: 0.8755 - val_precision: 0.0657 - val_recall: 0.1819 - val_auc: 0.5629\n",
      "Epoch 102/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2391 - tp: 9024.0000 - fp: 37809.0000 - tn: 421020.0000 - fn: 8317.0000 - bin_acc: 0.9031 - precision: 0.1927 - recall: 0.5204 - auc: 0.7400 - val_loss: 1.3774 - val_tp: 835.0000 - val_fp: 12208.0000 - val_tn: 102481.0000 - val_fn: 3518.0000 - val_bin_acc: 0.8679 - val_precision: 0.0640 - val_recall: 0.1918 - val_auc: 0.5645\n",
      "Epoch 103/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.2379 - tp: 9066.0000 - fp: 37299.0000 - tn: 421530.0000 - fn: 8275.0000 - bin_acc: 0.9043 - precision: 0.1955 - recall: 0.5228 - auc: 0.7406 - val_loss: 1.3778 - val_tp: 830.0000 - val_fp: 12446.0000 - val_tn: 102243.0000 - val_fn: 3523.0000 - val_bin_acc: 0.8659 - val_precision: 0.0625 - val_recall: 0.1907 - val_auc: 0.5649\n",
      "Epoch 104/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2370 - tp: 9075.0000 - fp: 37175.0000 - tn: 421654.0000 - fn: 8266.0000 - bin_acc: 0.9046 - precision: 0.1962 - recall: 0.5233 - auc: 0.7412 - val_loss: 1.3804 - val_tp: 991.0000 - val_fp: 15389.0000 - val_tn: 99300.0000 - val_fn: 3362.0000 - val_bin_acc: 0.8425 - val_precision: 0.0605 - val_recall: 0.2277 - val_auc: 0.5648\n",
      "Epoch 105/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2372 - tp: 9069.0000 - fp: 36859.0000 - tn: 421970.0000 - fn: 8272.0000 - bin_acc: 0.9052 - precision: 0.1975 - recall: 0.5230 - auc: 0.7414 - val_loss: 1.3756 - val_tp: 662.0000 - val_fp: 8748.0000 - val_tn: 105941.0000 - val_fn: 3691.0000 - val_bin_acc: 0.8955 - val_precision: 0.0704 - val_recall: 0.1521 - val_auc: 0.5581\n",
      "Epoch 106/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2371 - tp: 9119.0000 - fp: 37776.0000 - tn: 421053.0000 - fn: 8222.0000 - bin_acc: 0.9034 - precision: 0.1945 - recall: 0.5259 - auc: 0.7422 - val_loss: 1.3798 - val_tp: 804.0000 - val_fp: 12201.0000 - val_tn: 102488.0000 - val_fn: 3549.0000 - val_bin_acc: 0.8677 - val_precision: 0.0618 - val_recall: 0.1847 - val_auc: 0.5623\n",
      "Epoch 107/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2357 - tp: 9152.0000 - fp: 36715.0000 - tn: 422114.0000 - fn: 8189.0000 - bin_acc: 0.9057 - precision: 0.1995 - recall: 0.5278 - auc: 0.7437 - val_loss: 1.3774 - val_tp: 845.0000 - val_fp: 12209.0000 - val_tn: 102480.0000 - val_fn: 3508.0000 - val_bin_acc: 0.8680 - val_precision: 0.0647 - val_recall: 0.1941 - val_auc: 0.5623\n",
      "Epoch 108/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2367 - tp: 9112.0000 - fp: 37218.0000 - tn: 421611.0000 - fn: 8229.0000 - bin_acc: 0.9046 - precision: 0.1967 - recall: 0.5255 - auc: 0.7422 - val_loss: 1.3758 - val_tp: 764.0000 - val_fp: 10768.0000 - val_tn: 103921.0000 - val_fn: 3589.0000 - val_bin_acc: 0.8794 - val_precision: 0.0663 - val_recall: 0.1755 - val_auc: 0.5618\n",
      "Epoch 109/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2348 - tp: 9186.0000 - fp: 36709.0000 - tn: 422120.0000 - fn: 8155.0000 - bin_acc: 0.9058 - precision: 0.2002 - recall: 0.5297 - auc: 0.7440 - val_loss: 1.3780 - val_tp: 881.0000 - val_fp: 13073.0000 - val_tn: 101616.0000 - val_fn: 3472.0000 - val_bin_acc: 0.8610 - val_precision: 0.0631 - val_recall: 0.2024 - val_auc: 0.5643\n",
      "Epoch 110/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2355 - tp: 9146.0000 - fp: 36791.0000 - tn: 422038.0000 - fn: 8195.0000 - bin_acc: 0.9055 - precision: 0.1991 - recall: 0.5274 - auc: 0.7439 - val_loss: 1.3795 - val_tp: 842.0000 - val_fp: 12628.0000 - val_tn: 102061.0000 - val_fn: 3511.0000 - val_bin_acc: 0.8644 - val_precision: 0.0625 - val_recall: 0.1934 - val_auc: 0.5622\n",
      "Epoch 111/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2346 - tp: 9136.0000 - fp: 35772.0000 - tn: 423057.0000 - fn: 8205.0000 - bin_acc: 0.9076 - precision: 0.2034 - recall: 0.5268 - auc: 0.7443 - val_loss: 1.3794 - val_tp: 886.0000 - val_fp: 13626.0000 - val_tn: 101063.0000 - val_fn: 3467.0000 - val_bin_acc: 0.8564 - val_precision: 0.0611 - val_recall: 0.2035 - val_auc: 0.5662\n",
      "Epoch 112/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2339 - tp: 9249.0000 - fp: 36938.0000 - tn: 421891.0000 - fn: 8092.0000 - bin_acc: 0.9054 - precision: 0.2003 - recall: 0.5334 - auc: 0.7455 - val_loss: 1.3762 - val_tp: 798.0000 - val_fp: 11326.0000 - val_tn: 103363.0000 - val_fn: 3555.0000 - val_bin_acc: 0.8750 - val_precision: 0.0658 - val_recall: 0.1833 - val_auc: 0.5630\n",
      "Epoch 113/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2341 - tp: 9138.0000 - fp: 35714.0000 - tn: 423115.0000 - fn: 8203.0000 - bin_acc: 0.9078 - precision: 0.2037 - recall: 0.5270 - auc: 0.7454 - val_loss: 1.3770 - val_tp: 749.0000 - val_fp: 10763.0000 - val_tn: 103926.0000 - val_fn: 3604.0000 - val_bin_acc: 0.8793 - val_precision: 0.0651 - val_recall: 0.1721 - val_auc: 0.5603\n",
      "Epoch 114/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2348 - tp: 9155.0000 - fp: 36450.0000 - tn: 422379.0000 - fn: 8186.0000 - bin_acc: 0.9063 - precision: 0.2007 - recall: 0.5279 - auc: 0.7442 - val_loss: 1.3761 - val_tp: 717.0000 - val_fp: 10116.0000 - val_tn: 104573.0000 - val_fn: 3636.0000 - val_bin_acc: 0.8845 - val_precision: 0.0662 - val_recall: 0.1647 - val_auc: 0.5572\n",
      "Epoch 115/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2324 - tp: 9251.0000 - fp: 36084.0000 - tn: 422745.0000 - fn: 8090.0000 - bin_acc: 0.9072 - precision: 0.2041 - recall: 0.5335 - auc: 0.7465 - val_loss: 1.3775 - val_tp: 801.0000 - val_fp: 11612.0000 - val_tn: 103077.0000 - val_fn: 3552.0000 - val_bin_acc: 0.8726 - val_precision: 0.0645 - val_recall: 0.1840 - val_auc: 0.5612\n",
      "Epoch 116/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2337 - tp: 9220.0000 - fp: 36480.0000 - tn: 422349.0000 - fn: 8121.0000 - bin_acc: 0.9063 - precision: 0.2018 - recall: 0.5317 - auc: 0.7452 - val_loss: 1.3807 - val_tp: 893.0000 - val_fp: 13930.0000 - val_tn: 100759.0000 - val_fn: 3460.0000 - val_bin_acc: 0.8539 - val_precision: 0.0602 - val_recall: 0.2051 - val_auc: 0.5649\n",
      "Epoch 117/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2309 - tp: 9213.0000 - fp: 34383.0000 - tn: 424446.0000 - fn: 8128.0000 - bin_acc: 0.9107 - precision: 0.2113 - recall: 0.5313 - auc: 0.7482 - val_loss: 1.3770 - val_tp: 758.0000 - val_fp: 10899.0000 - val_tn: 103790.0000 - val_fn: 3595.0000 - val_bin_acc: 0.8782 - val_precision: 0.0650 - val_recall: 0.1741 - val_auc: 0.5596\n",
      "Epoch 118/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2336 - tp: 9207.0000 - fp: 36435.0000 - tn: 422394.0000 - fn: 8134.0000 - bin_acc: 0.9064 - precision: 0.2017 - recall: 0.5309 - auc: 0.7452 - val_loss: 1.3780 - val_tp: 728.0000 - val_fp: 10438.0000 - val_tn: 104251.0000 - val_fn: 3625.0000 - val_bin_acc: 0.8819 - val_precision: 0.0652 - val_recall: 0.1672 - val_auc: 0.5593\n",
      "Epoch 119/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2311 - tp: 9252.0000 - fp: 35165.0000 - tn: 423664.0000 - fn: 8089.0000 - bin_acc: 0.9092 - precision: 0.2083 - recall: 0.5335 - auc: 0.7482 - val_loss: 1.3785 - val_tp: 887.0000 - val_fp: 13256.0000 - val_tn: 101433.0000 - val_fn: 3466.0000 - val_bin_acc: 0.8595 - val_precision: 0.0627 - val_recall: 0.2038 - val_auc: 0.5633\n",
      "Epoch 120/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2316 - tp: 9279.0000 - fp: 36015.0000 - tn: 422814.0000 - fn: 8062.0000 - bin_acc: 0.9074 - precision: 0.2049 - recall: 0.5351 - auc: 0.7483 - val_loss: 1.3792 - val_tp: 891.0000 - val_fp: 13394.0000 - val_tn: 101295.0000 - val_fn: 3462.0000 - val_bin_acc: 0.8584 - val_precision: 0.0624 - val_recall: 0.2047 - val_auc: 0.5641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2317 - tp: 9278.0000 - fp: 36413.0000 - tn: 422416.0000 - fn: 8063.0000 - bin_acc: 0.9066 - precision: 0.2031 - recall: 0.5350 - auc: 0.7486 - val_loss: 1.3796 - val_tp: 951.0000 - val_fp: 14519.0000 - val_tn: 100170.0000 - val_fn: 3402.0000 - val_bin_acc: 0.8495 - val_precision: 0.0615 - val_recall: 0.2185 - val_auc: 0.5650\n",
      "Epoch 122/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2306 - tp: 9308.0000 - fp: 35976.0000 - tn: 422853.0000 - fn: 8033.0000 - bin_acc: 0.9076 - precision: 0.2055 - recall: 0.5368 - auc: 0.7495 - val_loss: 1.3769 - val_tp: 851.0000 - val_fp: 12315.0000 - val_tn: 102374.0000 - val_fn: 3502.0000 - val_bin_acc: 0.8671 - val_precision: 0.0646 - val_recall: 0.1955 - val_auc: 0.5616\n",
      "Epoch 123/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.2305 - tp: 9284.0000 - fp: 35281.0000 - tn: 423548.0000 - fn: 8057.0000 - bin_acc: 0.9090 - precision: 0.2083 - recall: 0.5354 - auc: 0.7490 - val_loss: 1.3797 - val_tp: 831.0000 - val_fp: 12826.0000 - val_tn: 101863.0000 - val_fn: 3522.0000 - val_bin_acc: 0.8627 - val_precision: 0.0608 - val_recall: 0.1909 - val_auc: 0.5604\n",
      "Epoch 124/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2310 - tp: 9280.0000 - fp: 35858.0000 - tn: 422971.0000 - fn: 8061.0000 - bin_acc: 0.9078 - precision: 0.2056 - recall: 0.5351 - auc: 0.7491 - val_loss: 1.3798 - val_tp: 937.0000 - val_fp: 14269.0000 - val_tn: 100420.0000 - val_fn: 3416.0000 - val_bin_acc: 0.8514 - val_precision: 0.0616 - val_recall: 0.2153 - val_auc: 0.5637\n",
      "Epoch 125/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2306 - tp: 9330.0000 - fp: 36091.0000 - tn: 422738.0000 - fn: 8011.0000 - bin_acc: 0.9074 - precision: 0.2054 - recall: 0.5380 - auc: 0.7488 - val_loss: 1.3766 - val_tp: 833.0000 - val_fp: 12039.0000 - val_tn: 102650.0000 - val_fn: 3520.0000 - val_bin_acc: 0.8693 - val_precision: 0.0647 - val_recall: 0.1914 - val_auc: 0.5632\n",
      "Epoch 126/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2297 - tp: 9310.0000 - fp: 35235.0000 - tn: 423594.0000 - fn: 8031.0000 - bin_acc: 0.9091 - precision: 0.2090 - recall: 0.5369 - auc: 0.7498 - val_loss: 1.3774 - val_tp: 749.0000 - val_fp: 10804.0000 - val_tn: 103885.0000 - val_fn: 3604.0000 - val_bin_acc: 0.8790 - val_precision: 0.0648 - val_recall: 0.1721 - val_auc: 0.5605\n",
      "Epoch 127/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2265 - tp: 9419.0000 - fp: 34545.0000 - tn: 424284.0000 - fn: 7922.0000 - bin_acc: 0.9108 - precision: 0.2142 - recall: 0.5432 - auc: 0.7527 - val_loss: 1.3776 - val_tp: 853.0000 - val_fp: 12383.0000 - val_tn: 102306.0000 - val_fn: 3500.0000 - val_bin_acc: 0.8666 - val_precision: 0.0644 - val_recall: 0.1960 - val_auc: 0.5631\n",
      "Epoch 128/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2279 - tp: 9332.0000 - fp: 34548.0000 - tn: 424281.0000 - fn: 8009.0000 - bin_acc: 0.9106 - precision: 0.2127 - recall: 0.5381 - auc: 0.7512 - val_loss: 1.3777 - val_tp: 799.0000 - val_fp: 11701.0000 - val_tn: 102988.0000 - val_fn: 3554.0000 - val_bin_acc: 0.8719 - val_precision: 0.0639 - val_recall: 0.1836 - val_auc: 0.5611\n",
      "Epoch 129/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2272 - tp: 9440.0000 - fp: 35206.0000 - tn: 423623.0000 - fn: 7901.0000 - bin_acc: 0.9095 - precision: 0.2114 - recall: 0.5444 - auc: 0.7529 - val_loss: 1.3756 - val_tp: 767.0000 - val_fp: 10479.0000 - val_tn: 104210.0000 - val_fn: 3586.0000 - val_bin_acc: 0.8818 - val_precision: 0.0682 - val_recall: 0.1762 - val_auc: 0.5585\n",
      "Epoch 130/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2279 - tp: 9412.0000 - fp: 35405.0000 - tn: 423424.0000 - fn: 7929.0000 - bin_acc: 0.9090 - precision: 0.2100 - recall: 0.5428 - auc: 0.7514 - val_loss: 1.3769 - val_tp: 798.0000 - val_fp: 11636.0000 - val_tn: 103053.0000 - val_fn: 3555.0000 - val_bin_acc: 0.8724 - val_precision: 0.0642 - val_recall: 0.1833 - val_auc: 0.5616\n",
      "Epoch 131/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2272 - tp: 9394.0000 - fp: 34788.0000 - tn: 424041.0000 - fn: 7947.0000 - bin_acc: 0.9103 - precision: 0.2126 - recall: 0.5417 - auc: 0.7531 - val_loss: 1.3784 - val_tp: 966.0000 - val_fp: 14716.0000 - val_tn: 99973.0000 - val_fn: 3387.0000 - val_bin_acc: 0.8479 - val_precision: 0.0616 - val_recall: 0.2219 - val_auc: 0.5668\n",
      "Epoch 132/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2273 - tp: 9398.0000 - fp: 34554.0000 - tn: 424275.0000 - fn: 7943.0000 - bin_acc: 0.9108 - precision: 0.2138 - recall: 0.5420 - auc: 0.7521 - val_loss: 1.3742 - val_tp: 567.0000 - val_fp: 6969.0000 - val_tn: 107720.0000 - val_fn: 3786.0000 - val_bin_acc: 0.9097 - val_precision: 0.0752 - val_recall: 0.1303 - val_auc: 0.5521\n",
      "Epoch 133/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2269 - tp: 9380.0000 - fp: 34235.0000 - tn: 424594.0000 - fn: 7961.0000 - bin_acc: 0.9114 - precision: 0.2151 - recall: 0.5409 - auc: 0.7524 - val_loss: 1.3772 - val_tp: 701.0000 - val_fp: 10163.0000 - val_tn: 104526.0000 - val_fn: 3652.0000 - val_bin_acc: 0.8839 - val_precision: 0.0645 - val_recall: 0.1610 - val_auc: 0.5584\n",
      "Epoch 134/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2262 - tp: 9418.0000 - fp: 34165.0000 - tn: 424664.0000 - fn: 7923.0000 - bin_acc: 0.9116 - precision: 0.2161 - recall: 0.5431 - auc: 0.7536 - val_loss: 1.3789 - val_tp: 952.0000 - val_fp: 14500.0000 - val_tn: 100189.0000 - val_fn: 3401.0000 - val_bin_acc: 0.8496 - val_precision: 0.0616 - val_recall: 0.2187 - val_auc: 0.5683\n",
      "Epoch 135/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2265 - tp: 9443.0000 - fp: 34937.0000 - tn: 423892.0000 - fn: 7898.0000 - bin_acc: 0.9100 - precision: 0.2128 - recall: 0.5445 - auc: 0.7527 - val_loss: 1.3778 - val_tp: 845.0000 - val_fp: 12524.0000 - val_tn: 102165.0000 - val_fn: 3508.0000 - val_bin_acc: 0.8653 - val_precision: 0.0632 - val_recall: 0.1941 - val_auc: 0.5635\n",
      "Epoch 136/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2267 - tp: 9397.0000 - fp: 34567.0000 - tn: 424262.0000 - fn: 7944.0000 - bin_acc: 0.9107 - precision: 0.2137 - recall: 0.5419 - auc: 0.7540 - val_loss: 1.3781 - val_tp: 720.0000 - val_fp: 10640.0000 - val_tn: 104049.0000 - val_fn: 3633.0000 - val_bin_acc: 0.8801 - val_precision: 0.0634 - val_recall: 0.1654 - val_auc: 0.5572\n",
      "Epoch 137/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2258 - tp: 9396.0000 - fp: 33462.0000 - tn: 425367.0000 - fn: 7945.0000 - bin_acc: 0.9130 - precision: 0.2192 - recall: 0.5418 - auc: 0.7538 - val_loss: 1.3787 - val_tp: 863.0000 - val_fp: 13110.0000 - val_tn: 101579.0000 - val_fn: 3490.0000 - val_bin_acc: 0.8606 - val_precision: 0.0618 - val_recall: 0.1983 - val_auc: 0.5637\n",
      "Epoch 138/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2244 - tp: 9473.0000 - fp: 33727.0000 - tn: 425102.0000 - fn: 7868.0000 - bin_acc: 0.9126 - precision: 0.2193 - recall: 0.5463 - auc: 0.7558 - val_loss: 1.3788 - val_tp: 830.0000 - val_fp: 12646.0000 - val_tn: 102043.0000 - val_fn: 3523.0000 - val_bin_acc: 0.8642 - val_precision: 0.0616 - val_recall: 0.1907 - val_auc: 0.5598\n",
      "Epoch 139/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2253 - tp: 9460.0000 - fp: 34266.0000 - tn: 424563.0000 - fn: 7881.0000 - bin_acc: 0.9115 - precision: 0.2163 - recall: 0.5455 - auc: 0.7541 - val_loss: 1.3789 - val_tp: 863.0000 - val_fp: 12979.0000 - val_tn: 101710.0000 - val_fn: 3490.0000 - val_bin_acc: 0.8617 - val_precision: 0.0623 - val_recall: 0.1983 - val_auc: 0.5604\n",
      "Epoch 140/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2256 - tp: 9474.0000 - fp: 34722.0000 - tn: 424107.0000 - fn: 7867.0000 - bin_acc: 0.9106 - precision: 0.2144 - recall: 0.5463 - auc: 0.7550 - val_loss: 1.3784 - val_tp: 728.0000 - val_fp: 10696.0000 - val_tn: 103993.0000 - val_fn: 3625.0000 - val_bin_acc: 0.8797 - val_precision: 0.0637 - val_recall: 0.1672 - val_auc: 0.5565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.2249 - tp: 9452.0000 - fp: 33790.0000 - tn: 425039.0000 - fn: 7889.0000 - bin_acc: 0.9125 - precision: 0.2186 - recall: 0.5451 - auc: 0.7554 - val_loss: 1.3792 - val_tp: 874.0000 - val_fp: 13275.0000 - val_tn: 101414.0000 - val_fn: 3479.0000 - val_bin_acc: 0.8593 - val_precision: 0.0618 - val_recall: 0.2008 - val_auc: 0.5614\n",
      "Epoch 142/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.2242 - tp: 9532.0000 - fp: 34733.0000 - tn: 424096.0000 - fn: 7809.0000 - bin_acc: 0.9107 - precision: 0.2153 - recall: 0.5497 - auc: 0.7549 - val_loss: 1.3793 - val_tp: 850.0000 - val_fp: 12801.0000 - val_tn: 101888.0000 - val_fn: 3503.0000 - val_bin_acc: 0.8630 - val_precision: 0.0623 - val_recall: 0.1953 - val_auc: 0.5636\n",
      "Epoch 143/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2236 - tp: 9479.0000 - fp: 33401.0000 - tn: 425428.0000 - fn: 7862.0000 - bin_acc: 0.9133 - precision: 0.2211 - recall: 0.5466 - auc: 0.7568 - val_loss: 1.3767 - val_tp: 752.0000 - val_fp: 10563.0000 - val_tn: 104126.0000 - val_fn: 3601.0000 - val_bin_acc: 0.8810 - val_precision: 0.0665 - val_recall: 0.1728 - val_auc: 0.5577\n",
      "Epoch 144/200\n",
      "476170/476170 [==============================] - 7s 14us/sample - loss: 1.2239 - tp: 9495.0000 - fp: 33628.0000 - tn: 425201.0000 - fn: 7846.0000 - bin_acc: 0.9129 - precision: 0.2202 - recall: 0.5475 - auc: 0.7560 - val_loss: 1.3775 - val_tp: 725.0000 - val_fp: 10563.0000 - val_tn: 104126.0000 - val_fn: 3628.0000 - val_bin_acc: 0.8808 - val_precision: 0.0642 - val_recall: 0.1666 - val_auc: 0.5581\n",
      "Epoch 145/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2227 - tp: 9478.0000 - fp: 32807.0000 - tn: 426022.0000 - fn: 7863.0000 - bin_acc: 0.9146 - precision: 0.2241 - recall: 0.5466 - auc: 0.7574 - val_loss: 1.3777 - val_tp: 716.0000 - val_fp: 10245.0000 - val_tn: 104444.0000 - val_fn: 3637.0000 - val_bin_acc: 0.8834 - val_precision: 0.0653 - val_recall: 0.1645 - val_auc: 0.5581\n",
      "Epoch 146/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2222 - tp: 9604.0000 - fp: 34302.0000 - tn: 424527.0000 - fn: 7737.0000 - bin_acc: 0.9117 - precision: 0.2187 - recall: 0.5538 - auc: 0.7582 - val_loss: 1.3782 - val_tp: 720.0000 - val_fp: 10377.0000 - val_tn: 104312.0000 - val_fn: 3633.0000 - val_bin_acc: 0.8823 - val_precision: 0.0649 - val_recall: 0.1654 - val_auc: 0.5553\n",
      "Epoch 147/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2232 - tp: 9558.0000 - fp: 34471.0000 - tn: 424358.0000 - fn: 7783.0000 - bin_acc: 0.9113 - precision: 0.2171 - recall: 0.5512 - auc: 0.7573 - val_loss: 1.3802 - val_tp: 874.0000 - val_fp: 13417.0000 - val_tn: 101272.0000 - val_fn: 3479.0000 - val_bin_acc: 0.8581 - val_precision: 0.0612 - val_recall: 0.2008 - val_auc: 0.5612\n",
      "Epoch 148/200\n",
      "476170/476170 [==============================] - 6s 14us/sample - loss: 1.2223 - tp: 9509.0000 - fp: 32937.0000 - tn: 425892.0000 - fn: 7832.0000 - bin_acc: 0.9144 - precision: 0.2240 - recall: 0.5484 - auc: 0.7586 - val_loss: 1.3811 - val_tp: 860.0000 - val_fp: 13538.0000 - val_tn: 101151.0000 - val_fn: 3493.0000 - val_bin_acc: 0.8569 - val_precision: 0.0597 - val_recall: 0.1976 - val_auc: 0.5591\n",
      "Epoch 149/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2222 - tp: 9534.0000 - fp: 33082.0000 - tn: 425747.0000 - fn: 7807.0000 - bin_acc: 0.9141 - precision: 0.2237 - recall: 0.5498 - auc: 0.7585 - val_loss: 1.3784 - val_tp: 861.0000 - val_fp: 12998.0000 - val_tn: 101691.0000 - val_fn: 3492.0000 - val_bin_acc: 0.8615 - val_precision: 0.0621 - val_recall: 0.1978 - val_auc: 0.5645\n",
      "Epoch 150/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2221 - tp: 9556.0000 - fp: 33580.0000 - tn: 425249.0000 - fn: 7785.0000 - bin_acc: 0.9131 - precision: 0.2215 - recall: 0.5511 - auc: 0.7591 - val_loss: 1.3799 - val_tp: 949.0000 - val_fp: 14564.0000 - val_tn: 100125.0000 - val_fn: 3404.0000 - val_bin_acc: 0.8491 - val_precision: 0.0612 - val_recall: 0.2180 - val_auc: 0.5660\n",
      "Epoch 151/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2219 - tp: 9544.0000 - fp: 33362.0000 - tn: 425467.0000 - fn: 7797.0000 - bin_acc: 0.9136 - precision: 0.2224 - recall: 0.5504 - auc: 0.7582 - val_loss: 1.3813 - val_tp: 956.0000 - val_fp: 15098.0000 - val_tn: 99591.0000 - val_fn: 3397.0000 - val_bin_acc: 0.8446 - val_precision: 0.0595 - val_recall: 0.2196 - val_auc: 0.5636\n",
      "Epoch 152/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2207 - tp: 9615.0000 - fp: 33679.0000 - tn: 425150.0000 - fn: 7726.0000 - bin_acc: 0.9130 - precision: 0.2221 - recall: 0.5545 - auc: 0.7599 - val_loss: 1.3832 - val_tp: 1007.0000 - val_fp: 16228.0000 - val_tn: 98461.0000 - val_fn: 3346.0000 - val_bin_acc: 0.8356 - val_precision: 0.0584 - val_recall: 0.2313 - val_auc: 0.5618\n",
      "Epoch 153/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2207 - tp: 9600.0000 - fp: 33043.0000 - tn: 425786.0000 - fn: 7741.0000 - bin_acc: 0.9143 - precision: 0.2251 - recall: 0.5536 - auc: 0.7601 - val_loss: 1.3775 - val_tp: 709.0000 - val_fp: 9997.0000 - val_tn: 104692.0000 - val_fn: 3644.0000 - val_bin_acc: 0.8854 - val_precision: 0.0662 - val_recall: 0.1629 - val_auc: 0.5561\n",
      "Epoch 154/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2211 - tp: 9588.0000 - fp: 33221.0000 - tn: 425608.0000 - fn: 7753.0000 - bin_acc: 0.9140 - precision: 0.2240 - recall: 0.5529 - auc: 0.7592 - val_loss: 1.3799 - val_tp: 751.0000 - val_fp: 11182.0000 - val_tn: 103507.0000 - val_fn: 3602.0000 - val_bin_acc: 0.8758 - val_precision: 0.0629 - val_recall: 0.1725 - val_auc: 0.5580\n",
      "Epoch 155/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2204 - tp: 9568.0000 - fp: 32810.0000 - tn: 426019.0000 - fn: 7773.0000 - bin_acc: 0.9148 - precision: 0.2258 - recall: 0.5518 - auc: 0.7609 - val_loss: 1.3814 - val_tp: 760.0000 - val_fp: 11935.0000 - val_tn: 102754.0000 - val_fn: 3593.0000 - val_bin_acc: 0.8696 - val_precision: 0.0599 - val_recall: 0.1746 - val_auc: 0.5567\n",
      "Epoch 156/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2196 - tp: 9662.0000 - fp: 33052.0000 - tn: 425777.0000 - fn: 7679.0000 - bin_acc: 0.9145 - precision: 0.2262 - recall: 0.5572 - auc: 0.7611 - val_loss: 1.3783 - val_tp: 839.0000 - val_fp: 12435.0000 - val_tn: 102254.0000 - val_fn: 3514.0000 - val_bin_acc: 0.8660 - val_precision: 0.0632 - val_recall: 0.1927 - val_auc: 0.5602\n",
      "Epoch 157/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2203 - tp: 9588.0000 - fp: 32746.0000 - tn: 426083.0000 - fn: 7753.0000 - bin_acc: 0.9149 - precision: 0.2265 - recall: 0.5529 - auc: 0.7612 - val_loss: 1.3805 - val_tp: 905.0000 - val_fp: 14112.0000 - val_tn: 100577.0000 - val_fn: 3448.0000 - val_bin_acc: 0.8525 - val_precision: 0.0603 - val_recall: 0.2079 - val_auc: 0.5613\n",
      "Epoch 158/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2206 - tp: 9621.0000 - fp: 33773.0000 - tn: 425056.0000 - fn: 7720.0000 - bin_acc: 0.9129 - precision: 0.2217 - recall: 0.5548 - auc: 0.7601 - val_loss: 1.3774 - val_tp: 545.0000 - val_fp: 7391.0000 - val_tn: 107298.0000 - val_fn: 3808.0000 - val_bin_acc: 0.9059 - val_precision: 0.0687 - val_recall: 0.1252 - val_auc: 0.5482\n",
      "Epoch 159/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2196 - tp: 9612.0000 - fp: 32446.0000 - tn: 426383.0000 - fn: 7729.0000 - bin_acc: 0.9156 - precision: 0.2285 - recall: 0.5543 - auc: 0.7611 - val_loss: 1.3803 - val_tp: 763.0000 - val_fp: 11815.0000 - val_tn: 102874.0000 - val_fn: 3590.0000 - val_bin_acc: 0.8706 - val_precision: 0.0607 - val_recall: 0.1753 - val_auc: 0.5579\n",
      "Epoch 160/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2206 - tp: 9623.0000 - fp: 33795.0000 - tn: 425034.0000 - fn: 7718.0000 - bin_acc: 0.9128 - precision: 0.2216 - recall: 0.5549 - auc: 0.7604 - val_loss: 1.3777 - val_tp: 618.0000 - val_fp: 8592.0000 - val_tn: 106097.0000 - val_fn: 3735.0000 - val_bin_acc: 0.8964 - val_precision: 0.0671 - val_recall: 0.1420 - val_auc: 0.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2190 - tp: 9582.0000 - fp: 31779.0000 - tn: 427050.0000 - fn: 7759.0000 - bin_acc: 0.9170 - precision: 0.2317 - recall: 0.5526 - auc: 0.7608 - val_loss: 1.3809 - val_tp: 863.0000 - val_fp: 13546.0000 - val_tn: 101143.0000 - val_fn: 3490.0000 - val_bin_acc: 0.8569 - val_precision: 0.0599 - val_recall: 0.1983 - val_auc: 0.5613\n",
      "Epoch 162/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2183 - tp: 9683.0000 - fp: 32522.0000 - tn: 426307.0000 - fn: 7658.0000 - bin_acc: 0.9156 - precision: 0.2294 - recall: 0.5584 - auc: 0.7617 - val_loss: 1.3804 - val_tp: 876.0000 - val_fp: 13417.0000 - val_tn: 101272.0000 - val_fn: 3477.0000 - val_bin_acc: 0.8581 - val_precision: 0.0613 - val_recall: 0.2012 - val_auc: 0.5605\n",
      "Epoch 163/200\n",
      "476170/476170 [==============================] - 6s 12us/sample - loss: 1.2184 - tp: 9698.0000 - fp: 32955.0000 - tn: 425874.0000 - fn: 7643.0000 - bin_acc: 0.9147 - precision: 0.2274 - recall: 0.5593 - auc: 0.7626 - val_loss: 1.3797 - val_tp: 705.0000 - val_fp: 10381.0000 - val_tn: 104308.0000 - val_fn: 3648.0000 - val_bin_acc: 0.8822 - val_precision: 0.0636 - val_recall: 0.1620 - val_auc: 0.5544\n",
      "Epoch 164/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2177 - tp: 9653.0000 - fp: 31926.0000 - tn: 426903.0000 - fn: 7688.0000 - bin_acc: 0.9168 - precision: 0.2322 - recall: 0.5567 - auc: 0.7629 - val_loss: 1.3797 - val_tp: 717.0000 - val_fp: 10769.0000 - val_tn: 103920.0000 - val_fn: 3636.0000 - val_bin_acc: 0.8790 - val_precision: 0.0624 - val_recall: 0.1647 - val_auc: 0.5558\n",
      "Epoch 165/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2181 - tp: 9634.0000 - fp: 32091.0000 - tn: 426738.0000 - fn: 7707.0000 - bin_acc: 0.9164 - precision: 0.2309 - recall: 0.5556 - auc: 0.7629 - val_loss: 1.3816 - val_tp: 920.0000 - val_fp: 14376.0000 - val_tn: 100313.0000 - val_fn: 3433.0000 - val_bin_acc: 0.8504 - val_precision: 0.0601 - val_recall: 0.2113 - val_auc: 0.5629\n",
      "Epoch 166/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2178 - tp: 9709.0000 - fp: 32821.0000 - tn: 426008.0000 - fn: 7632.0000 - bin_acc: 0.9150 - precision: 0.2283 - recall: 0.5599 - auc: 0.7634 - val_loss: 1.3788 - val_tp: 848.0000 - val_fp: 12630.0000 - val_tn: 102059.0000 - val_fn: 3505.0000 - val_bin_acc: 0.8645 - val_precision: 0.0629 - val_recall: 0.1948 - val_auc: 0.5619\n",
      "Epoch 167/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2176 - tp: 9722.0000 - fp: 32673.0000 - tn: 426156.0000 - fn: 7619.0000 - bin_acc: 0.9154 - precision: 0.2293 - recall: 0.5606 - auc: 0.7641 - val_loss: 1.3814 - val_tp: 907.0000 - val_fp: 14202.0000 - val_tn: 100487.0000 - val_fn: 3446.0000 - val_bin_acc: 0.8517 - val_precision: 0.0600 - val_recall: 0.2084 - val_auc: 0.5633\n",
      "Epoch 168/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2170 - tp: 9722.0000 - fp: 32417.0000 - tn: 426412.0000 - fn: 7619.0000 - bin_acc: 0.9159 - precision: 0.2307 - recall: 0.5606 - auc: 0.7643 - val_loss: 1.3791 - val_tp: 783.0000 - val_fp: 11630.0000 - val_tn: 103059.0000 - val_fn: 3570.0000 - val_bin_acc: 0.8723 - val_precision: 0.0631 - val_recall: 0.1799 - val_auc: 0.5578\n",
      "Epoch 169/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2162 - tp: 9729.0000 - fp: 31861.0000 - tn: 426968.0000 - fn: 7612.0000 - bin_acc: 0.9171 - precision: 0.2339 - recall: 0.5610 - auc: 0.7647 - val_loss: 1.3792 - val_tp: 822.0000 - val_fp: 12276.0000 - val_tn: 102413.0000 - val_fn: 3531.0000 - val_bin_acc: 0.8672 - val_precision: 0.0628 - val_recall: 0.1888 - val_auc: 0.5603\n",
      "Epoch 170/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2161 - tp: 9782.0000 - fp: 32315.0000 - tn: 426514.0000 - fn: 7559.0000 - bin_acc: 0.9163 - precision: 0.2324 - recall: 0.5641 - auc: 0.7642 - val_loss: 1.3804 - val_tp: 830.0000 - val_fp: 12675.0000 - val_tn: 102014.0000 - val_fn: 3523.0000 - val_bin_acc: 0.8639 - val_precision: 0.0615 - val_recall: 0.1907 - val_auc: 0.5602\n",
      "Epoch 171/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2162 - tp: 9752.0000 - fp: 32381.0000 - tn: 426448.0000 - fn: 7589.0000 - bin_acc: 0.9161 - precision: 0.2315 - recall: 0.5624 - auc: 0.7647 - val_loss: 1.3802 - val_tp: 796.0000 - val_fp: 12143.0000 - val_tn: 102546.0000 - val_fn: 3557.0000 - val_bin_acc: 0.8681 - val_precision: 0.0615 - val_recall: 0.1829 - val_auc: 0.5598\n",
      "Epoch 172/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2163 - tp: 9777.0000 - fp: 32875.0000 - tn: 425954.0000 - fn: 7564.0000 - bin_acc: 0.9151 - precision: 0.2292 - recall: 0.5638 - auc: 0.7657 - val_loss: 1.3815 - val_tp: 734.0000 - val_fp: 11316.0000 - val_tn: 103373.0000 - val_fn: 3619.0000 - val_bin_acc: 0.8745 - val_precision: 0.0609 - val_recall: 0.1686 - val_auc: 0.5564\n",
      "Epoch 173/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2154 - tp: 9757.0000 - fp: 31798.0000 - tn: 427031.0000 - fn: 7584.0000 - bin_acc: 0.9173 - precision: 0.2348 - recall: 0.5627 - auc: 0.7654 - val_loss: 1.3832 - val_tp: 862.0000 - val_fp: 13888.0000 - val_tn: 100801.0000 - val_fn: 3491.0000 - val_bin_acc: 0.8540 - val_precision: 0.0584 - val_recall: 0.1980 - val_auc: 0.5594\n",
      "Epoch 174/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2152 - tp: 9726.0000 - fp: 31350.0000 - tn: 427479.0000 - fn: 7615.0000 - bin_acc: 0.9182 - precision: 0.2368 - recall: 0.5609 - auc: 0.7664 - val_loss: 1.3803 - val_tp: 803.0000 - val_fp: 12174.0000 - val_tn: 102515.0000 - val_fn: 3550.0000 - val_bin_acc: 0.8679 - val_precision: 0.0619 - val_recall: 0.1845 - val_auc: 0.5573\n",
      "Epoch 175/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2152 - tp: 9812.0000 - fp: 32238.0000 - tn: 426591.0000 - fn: 7529.0000 - bin_acc: 0.9165 - precision: 0.2333 - recall: 0.5658 - auc: 0.7653 - val_loss: 1.3798 - val_tp: 818.0000 - val_fp: 12419.0000 - val_tn: 102270.0000 - val_fn: 3535.0000 - val_bin_acc: 0.8660 - val_precision: 0.0618 - val_recall: 0.1879 - val_auc: 0.5604\n",
      "Epoch 176/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2148 - tp: 9792.0000 - fp: 31678.0000 - tn: 427151.0000 - fn: 7549.0000 - bin_acc: 0.9176 - precision: 0.2361 - recall: 0.5647 - auc: 0.7663 - val_loss: 1.3776 - val_tp: 651.0000 - val_fp: 8841.0000 - val_tn: 105848.0000 - val_fn: 3702.0000 - val_bin_acc: 0.8946 - val_precision: 0.0686 - val_recall: 0.1496 - val_auc: 0.5542\n",
      "Epoch 177/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2150 - tp: 9777.0000 - fp: 31851.0000 - tn: 426978.0000 - fn: 7564.0000 - bin_acc: 0.9172 - precision: 0.2349 - recall: 0.5638 - auc: 0.7664 - val_loss: 1.3792 - val_tp: 720.0000 - val_fp: 10685.0000 - val_tn: 104004.0000 - val_fn: 3633.0000 - val_bin_acc: 0.8797 - val_precision: 0.0631 - val_recall: 0.1654 - val_auc: 0.5582\n",
      "Epoch 178/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2146 - tp: 9832.0000 - fp: 32167.0000 - tn: 426662.0000 - fn: 7509.0000 - bin_acc: 0.9167 - precision: 0.2341 - recall: 0.5670 - auc: 0.7665 - val_loss: 1.3798 - val_tp: 928.0000 - val_fp: 14192.0000 - val_tn: 100497.0000 - val_fn: 3425.0000 - val_bin_acc: 0.8520 - val_precision: 0.0614 - val_recall: 0.2132 - val_auc: 0.5629\n",
      "Epoch 179/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2147 - tp: 9780.0000 - fp: 31507.0000 - tn: 427322.0000 - fn: 7561.0000 - bin_acc: 0.9180 - precision: 0.2369 - recall: 0.5640 - auc: 0.7665 - val_loss: 1.3785 - val_tp: 743.0000 - val_fp: 10963.0000 - val_tn: 103726.0000 - val_fn: 3610.0000 - val_bin_acc: 0.8776 - val_precision: 0.0635 - val_recall: 0.1707 - val_auc: 0.5578\n",
      "Epoch 180/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2137 - tp: 9859.0000 - fp: 31923.0000 - tn: 426906.0000 - fn: 7482.0000 - bin_acc: 0.9172 - precision: 0.2360 - recall: 0.5685 - auc: 0.7672 - val_loss: 1.3800 - val_tp: 747.0000 - val_fp: 11238.0000 - val_tn: 103451.0000 - val_fn: 3606.0000 - val_bin_acc: 0.8753 - val_precision: 0.0623 - val_recall: 0.1716 - val_auc: 0.5561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "476170/476170 [==============================] - 6s 14us/sample - loss: 1.2130 - tp: 9881.0000 - fp: 31607.0000 - tn: 427222.0000 - fn: 7460.0000 - bin_acc: 0.9180 - precision: 0.2382 - recall: 0.5698 - auc: 0.7683 - val_loss: 1.3803 - val_tp: 813.0000 - val_fp: 12549.0000 - val_tn: 102140.0000 - val_fn: 3540.0000 - val_bin_acc: 0.8648 - val_precision: 0.0608 - val_recall: 0.1868 - val_auc: 0.5580\n",
      "Epoch 182/200\n",
      "476170/476170 [==============================] - 6s 14us/sample - loss: 1.2130 - tp: 9808.0000 - fp: 30707.0000 - tn: 428122.0000 - fn: 7533.0000 - bin_acc: 0.9197 - precision: 0.2421 - recall: 0.5656 - auc: 0.7689 - val_loss: 1.3815 - val_tp: 851.0000 - val_fp: 13184.0000 - val_tn: 101505.0000 - val_fn: 3502.0000 - val_bin_acc: 0.8598 - val_precision: 0.0606 - val_recall: 0.1955 - val_auc: 0.5588\n",
      "Epoch 183/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2132 - tp: 9829.0000 - fp: 31272.0000 - tn: 427557.0000 - fn: 7512.0000 - bin_acc: 0.9186 - precision: 0.2391 - recall: 0.5668 - auc: 0.7679 - val_loss: 1.3838 - val_tp: 904.0000 - val_fp: 14481.0000 - val_tn: 100208.0000 - val_fn: 3449.0000 - val_bin_acc: 0.8494 - val_precision: 0.0588 - val_recall: 0.2077 - val_auc: 0.5580\n",
      "Epoch 184/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2123 - tp: 9819.0000 - fp: 30427.0000 - tn: 428402.0000 - fn: 7522.0000 - bin_acc: 0.9203 - precision: 0.2440 - recall: 0.5662 - auc: 0.7689 - val_loss: 1.3782 - val_tp: 648.0000 - val_fp: 9269.0000 - val_tn: 105420.0000 - val_fn: 3705.0000 - val_bin_acc: 0.8910 - val_precision: 0.0653 - val_recall: 0.1489 - val_auc: 0.5534\n",
      "Epoch 185/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2136 - tp: 9804.0000 - fp: 31313.0000 - tn: 427516.0000 - fn: 7537.0000 - bin_acc: 0.9184 - precision: 0.2384 - recall: 0.5654 - auc: 0.7687 - val_loss: 1.3788 - val_tp: 785.0000 - val_fp: 11664.0000 - val_tn: 103025.0000 - val_fn: 3568.0000 - val_bin_acc: 0.8720 - val_precision: 0.0631 - val_recall: 0.1803 - val_auc: 0.5575\n",
      "Epoch 186/200\n",
      "476170/476170 [==============================] - 6s 14us/sample - loss: 1.2129 - tp: 9868.0000 - fp: 31646.0000 - tn: 427183.0000 - fn: 7473.0000 - bin_acc: 0.9178 - precision: 0.2377 - recall: 0.5691 - auc: 0.7689 - val_loss: 1.3810 - val_tp: 936.0000 - val_fp: 14666.0000 - val_tn: 100023.0000 - val_fn: 3417.0000 - val_bin_acc: 0.8481 - val_precision: 0.0600 - val_recall: 0.2150 - val_auc: 0.5630\n",
      "Epoch 187/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2117 - tp: 9858.0000 - fp: 30576.0000 - tn: 428253.0000 - fn: 7483.0000 - bin_acc: 0.9201 - precision: 0.2438 - recall: 0.5685 - auc: 0.7699 - val_loss: 1.3799 - val_tp: 844.0000 - val_fp: 12732.0000 - val_tn: 101957.0000 - val_fn: 3509.0000 - val_bin_acc: 0.8636 - val_precision: 0.0622 - val_recall: 0.1939 - val_auc: 0.5578\n",
      "Epoch 188/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2127 - tp: 9834.0000 - fp: 31065.0000 - tn: 427764.0000 - fn: 7507.0000 - bin_acc: 0.9190 - precision: 0.2404 - recall: 0.5671 - auc: 0.7688 - val_loss: 1.3799 - val_tp: 751.0000 - val_fp: 11403.0000 - val_tn: 103286.0000 - val_fn: 3602.0000 - val_bin_acc: 0.8740 - val_precision: 0.0618 - val_recall: 0.1725 - val_auc: 0.5564\n",
      "Epoch 189/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2121 - tp: 9872.0000 - fp: 31086.0000 - tn: 427743.0000 - fn: 7469.0000 - bin_acc: 0.9190 - precision: 0.2410 - recall: 0.5693 - auc: 0.7693 - val_loss: 1.3794 - val_tp: 729.0000 - val_fp: 10698.0000 - val_tn: 103991.0000 - val_fn: 3624.0000 - val_bin_acc: 0.8797 - val_precision: 0.0638 - val_recall: 0.1675 - val_auc: 0.5537\n",
      "Epoch 190/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2116 - tp: 9879.0000 - fp: 31136.0000 - tn: 427693.0000 - fn: 7462.0000 - bin_acc: 0.9189 - precision: 0.2409 - recall: 0.5697 - auc: 0.7700 - val_loss: 1.3790 - val_tp: 800.0000 - val_fp: 11762.0000 - val_tn: 102927.0000 - val_fn: 3553.0000 - val_bin_acc: 0.8713 - val_precision: 0.0637 - val_recall: 0.1838 - val_auc: 0.5573\n",
      "Epoch 191/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2106 - tp: 9912.0000 - fp: 30540.0000 - tn: 428289.0000 - fn: 7429.0000 - bin_acc: 0.9203 - precision: 0.2450 - recall: 0.5716 - auc: 0.7706 - val_loss: 1.3792 - val_tp: 712.0000 - val_fp: 10575.0000 - val_tn: 104114.0000 - val_fn: 3641.0000 - val_bin_acc: 0.8806 - val_precision: 0.0631 - val_recall: 0.1636 - val_auc: 0.5566\n",
      "Epoch 192/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2112 - tp: 9870.0000 - fp: 30623.0000 - tn: 428206.0000 - fn: 7471.0000 - bin_acc: 0.9200 - precision: 0.2437 - recall: 0.5692 - auc: 0.7703 - val_loss: 1.3783 - val_tp: 726.0000 - val_fp: 10455.0000 - val_tn: 104234.0000 - val_fn: 3627.0000 - val_bin_acc: 0.8817 - val_precision: 0.0649 - val_recall: 0.1668 - val_auc: 0.5567\n",
      "Epoch 193/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2109 - tp: 9907.0000 - fp: 30810.0000 - tn: 428019.0000 - fn: 7434.0000 - bin_acc: 0.9197 - precision: 0.2433 - recall: 0.5713 - auc: 0.7714 - val_loss: 1.3820 - val_tp: 750.0000 - val_fp: 11830.0000 - val_tn: 102859.0000 - val_fn: 3603.0000 - val_bin_acc: 0.8704 - val_precision: 0.0596 - val_recall: 0.1723 - val_auc: 0.5549\n",
      "Epoch 194/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2104 - tp: 9890.0000 - fp: 30201.0000 - tn: 428628.0000 - fn: 7451.0000 - bin_acc: 0.9209 - precision: 0.2467 - recall: 0.5703 - auc: 0.7714 - val_loss: 1.3797 - val_tp: 760.0000 - val_fp: 11311.0000 - val_tn: 103378.0000 - val_fn: 3593.0000 - val_bin_acc: 0.8748 - val_precision: 0.0630 - val_recall: 0.1746 - val_auc: 0.5552\n",
      "Epoch 195/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2094 - tp: 9915.0000 - fp: 29813.0000 - tn: 429016.0000 - fn: 7426.0000 - bin_acc: 0.9218 - precision: 0.2496 - recall: 0.5718 - auc: 0.7716 - val_loss: 1.3788 - val_tp: 808.0000 - val_fp: 11859.0000 - val_tn: 102830.0000 - val_fn: 3545.0000 - val_bin_acc: 0.8706 - val_precision: 0.0638 - val_recall: 0.1856 - val_auc: 0.5590\n",
      "Epoch 196/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2104 - tp: 9911.0000 - fp: 30295.0000 - tn: 428534.0000 - fn: 7430.0000 - bin_acc: 0.9208 - precision: 0.2465 - recall: 0.5715 - auc: 0.7711 - val_loss: 1.3810 - val_tp: 767.0000 - val_fp: 11922.0000 - val_tn: 102767.0000 - val_fn: 3586.0000 - val_bin_acc: 0.8697 - val_precision: 0.0604 - val_recall: 0.1762 - val_auc: 0.5546\n",
      "Epoch 197/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2094 - tp: 9960.0000 - fp: 30367.0000 - tn: 428462.0000 - fn: 7381.0000 - bin_acc: 0.9207 - precision: 0.2470 - recall: 0.5744 - auc: 0.7718 - val_loss: 1.3821 - val_tp: 833.0000 - val_fp: 13157.0000 - val_tn: 101532.0000 - val_fn: 3520.0000 - val_bin_acc: 0.8599 - val_precision: 0.0595 - val_recall: 0.1914 - val_auc: 0.5592\n",
      "Epoch 198/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2105 - tp: 9874.0000 - fp: 30032.0000 - tn: 428797.0000 - fn: 7467.0000 - bin_acc: 0.9212 - precision: 0.2474 - recall: 0.5694 - auc: 0.7711 - val_loss: 1.3776 - val_tp: 686.0000 - val_fp: 9607.0000 - val_tn: 105082.0000 - val_fn: 3667.0000 - val_bin_acc: 0.8885 - val_precision: 0.0666 - val_recall: 0.1576 - val_auc: 0.5520\n",
      "Epoch 199/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2092 - tp: 9953.0000 - fp: 30144.0000 - tn: 428685.0000 - fn: 7388.0000 - bin_acc: 0.9212 - precision: 0.2482 - recall: 0.5740 - auc: 0.7720 - val_loss: 1.3810 - val_tp: 784.0000 - val_fp: 12002.0000 - val_tn: 102687.0000 - val_fn: 3569.0000 - val_bin_acc: 0.8692 - val_precision: 0.0613 - val_recall: 0.1801 - val_auc: 0.5575\n",
      "Epoch 200/200\n",
      "476170/476170 [==============================] - 6s 13us/sample - loss: 1.2086 - tp: 9945.0000 - fp: 29801.0000 - tn: 429028.0000 - fn: 7396.0000 - bin_acc: 0.9219 - precision: 0.2502 - recall: 0.5735 - auc: 0.7727 - val_loss: 1.3779 - val_tp: 659.0000 - val_fp: 9253.0000 - val_tn: 105436.0000 - val_fn: 3694.0000 - val_bin_acc: 0.8912 - val_precision: 0.0665 - val_recall: 0.1514 - val_auc: 0.5523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8503a02e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepFM(cfg)\n",
    "# model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "model.compile(optimizer = 'adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=METRICS)\n",
    "model.fit(train_input, train_y, epochs=cfg['epoch'], batch_size=cfg['batch'], shuffle=True, class_weight=cfg['class_weight'],\n",
    "          verbose=1, validation_data=(validate_input, validate_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_for_vali = model.predict(validate_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.222248e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.936440e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.617738e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.669779e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.181231e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.864003e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.198649e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.999912e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.278255e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.960464e-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.690333e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.940697e-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.999064e-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8.371919e-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict  true\n",
       "0   6.222248e-01     0\n",
       "1   0.000000e+00     0\n",
       "2   3.936440e-03     0\n",
       "3   5.617738e-05     0\n",
       "4   1.669779e-01     0\n",
       "5   4.181231e-01     0\n",
       "6   0.000000e+00     0\n",
       "7   2.864003e-05     0\n",
       "8   0.000000e+00     0\n",
       "9   1.198649e-04     0\n",
       "10  0.000000e+00     0\n",
       "11  0.000000e+00     0\n",
       "12  9.999912e-01     0\n",
       "13  3.278255e-07     0\n",
       "14  5.960464e-08     0\n",
       "15  0.000000e+00     0\n",
       "16  0.000000e+00     0\n",
       "17  0.000000e+00     0\n",
       "18  0.000000e+00     0\n",
       "19  5.690333e-01     0\n",
       "20  0.000000e+00     0\n",
       "21  0.000000e+00     0\n",
       "22  8.940697e-08     0\n",
       "23  0.000000e+00     0\n",
       "24  0.000000e+00     0\n",
       "25  9.999064e-01     0\n",
       "26  0.000000e+00     0\n",
       "27  0.000000e+00     0\n",
       "28  0.000000e+00     0\n",
       "29  0.000000e+00     0\n",
       "30  0.000000e+00     0\n",
       "31  8.371919e-02     0\n",
       "32  0.000000e+00     0\n",
       "33  0.000000e+00     0\n",
       "34  0.000000e+00     0\n",
       "35  0.000000e+00     0\n",
       "36  0.000000e+00     0\n",
       "37  0.000000e+00     0\n",
       "38  0.000000e+00     0\n",
       "39  0.000000e+00     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_pd = pd.concat([pd.DataFrame(predict_for_vali), pd.DataFrame(validate_y)], axis=1)\n",
    "check_pd.columns = ['predict', 'true']\n",
    "check_pd.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = check_pd.sort_values(by=[\"true\",\"predict\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21650</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52771</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64500</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81446</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113761</th>\n",
       "      <td>0.997498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68538</th>\n",
       "      <td>0.997455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113254</th>\n",
       "      <td>0.997418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108988</th>\n",
       "      <td>0.997367</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52658</th>\n",
       "      <td>0.997296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict  true\n",
       "21650   1.000000     1\n",
       "48182   1.000000     1\n",
       "52771   1.000000     1\n",
       "64500   1.000000     1\n",
       "81446   1.000000     1\n",
       "...          ...   ...\n",
       "113761  0.997498     1\n",
       "68538   0.997455     1\n",
       "113254  0.997418     1\n",
       "108988  0.997367     1\n",
       "52658   0.997296     1\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sort = check_pd.sort_values(by=[\"predict\",\"true\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21650</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52771</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64500</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81446</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45880</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85395</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87222</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111700</th>\n",
       "      <td>0.999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65277</th>\n",
       "      <td>0.999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict  true\n",
       "21650   1.000000     1\n",
       "48182   1.000000     1\n",
       "52771   1.000000     1\n",
       "64500   1.000000     1\n",
       "81446   1.000000     1\n",
       "...          ...   ...\n",
       "45880   0.999997     0\n",
       "85395   0.999997     0\n",
       "87222   0.999997     0\n",
       "111700  0.999997     0\n",
       "65277   0.999996     0\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sort[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
