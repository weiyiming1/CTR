{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./train.csv\"\n",
    "test_file = \"./test.csv\"\n",
    "num_cols = [\"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\"]\n",
    "ignore_cols = [\"id\", \"target\", \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\", \"ps_calc_05\", \n",
    "               \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\", \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \n",
    "               \"ps_calc_12\", \"ps_calc_13\", \"ps_calc_14\",\"ps_calc_15_bin\", \"ps_calc_16_bin\", \n",
    "               \"ps_calc_17_bin\",\"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"feature_size\": None,\n",
    "    \"field_size\": None,\n",
    "    \"embed_size\": 128,\n",
    "    \"deep_nn\":[256,256],\n",
    "    \"dropout_fm\": 0,\n",
    "    \"dropout_deep\": 0.2,\n",
    "    \"output_bias\": None,\n",
    "    \"epoch\": 200,\n",
    "    \"batch\":10000,\n",
    "    \"split\": 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overview(cfg):    \n",
    "    dfTrain = pd.read_csv(train_file)\n",
    "    dfTest = pd.read_csv(test_file)\n",
    "    df = pd.concat([dfTrain,dfTest], sort=False)\n",
    "\n",
    "    field_size = len(df.columns) - len(ignore_cols)\n",
    "    feature_dict = {}\n",
    "    feature_size = 0\n",
    "    for col in df.columns:\n",
    "        if col in ignore_cols:\n",
    "            continue\n",
    "        elif col in num_cols:\n",
    "            feature_dict[col] = feature_size\n",
    "            feature_size += 1\n",
    "        else:\n",
    "            unique_val = df[col].unique()\n",
    "            feature_dict[col] = dict(zip(unique_val,range(feature_size,len(unique_val) + feature_size)))\n",
    "            feature_size += len(unique_val)\n",
    "    \n",
    "    cfg['field_size'] = field_size\n",
    "    cfg['feature_size'] = feature_size\n",
    "    return dfTrain, feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain, feature_dict = overview(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_df, feature_dict, cfg):\n",
    "    label_df = train_df[['target']]  \n",
    "    neg, pos = np.bincount(label_df.values.flatten())\n",
    "    cfg['output_bias'] = np.log([pos/neg])\n",
    "    train_df.drop(['target','id'],axis=1,inplace=True)\n",
    "    feature_idx = train_df.copy()\n",
    "    feature_val = train_df.copy()\n",
    "    for col in feature_idx.columns:\n",
    "        if col in ignore_cols:\n",
    "            feature_idx.drop(col,axis=1,inplace=True)\n",
    "            feature_val.drop(col,axis=1,inplace=True)\n",
    "            continue\n",
    "        elif col in num_cols:\n",
    "            feature_idx[col] = feature_dict[col]\n",
    "        else:\n",
    "            feature_idx[col] = feature_idx[col].map(feature_dict[col])\n",
    "            feature_val[col] = 1      \n",
    "            \n",
    "    train_idx_df, test_idx_df = train_test_split(feature_idx, test_size=cfg[\"split\"])\n",
    "    train_val_df, test_val_df = train_test_split(feature_val, test_size=cfg[\"split\"])\n",
    "    train_label_df, test_label_df = train_test_split(label_df, test_size=cfg[\"split\"])\n",
    "    \n",
    "    train_idx_df, validate_idx_df = train_test_split(train_idx_df, test_size=cfg[\"split\"])\n",
    "    train_val_df, validate_val_df = train_test_split(train_val_df, test_size=cfg[\"split\"])\n",
    "    train_label_df, validate_label_df = train_test_split(train_label_df, test_size=cfg[\"split\"])\n",
    "    \n",
    "    train_input = [train_idx_df.values, train_val_df.values]\n",
    "    train_label = np.array(train_label_df['target'])\n",
    "    bool_train_labels = train_label != 0\n",
    "    \n",
    "    validate_input = [validate_idx_df.values, validate_val_df.values]\n",
    "    validate_label = validate_label_df.values\n",
    "    \n",
    "    test_input = [test_idx_df.values, test_val_df.values]\n",
    "    test_label = test_label_df.values\n",
    "    \n",
    "    return train_input, train_label, bool_train_labels, validate_input, validate_label, test_input, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_label, bool_train_labels, validate_input, validate_label, test_input, test_label = preprocess(dfTrain, feature_dict, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(train_input, train_label, bool_train_labels):\n",
    "    pos_idx = train_input[0][bool_train_labels]\n",
    "    neg_idx = train_input[0][~bool_train_labels]\n",
    "    pos_val = train_input[1][bool_train_labels]\n",
    "    neg_val = train_input[1][~bool_train_labels]\n",
    "    pos_label = train_label[bool_train_labels]\n",
    "    neg_label = train_label[~bool_train_labels]\n",
    "    \n",
    "    ids = np.arange(len(pos_idx))\n",
    "    choices = np.random.choice(ids, len(neg_idx))\n",
    "    \n",
    "    res_pos_idx = pos_idx[choices]\n",
    "    res_pos_val = pos_val[choices]\n",
    "    res_pos_label = pos_label[choices]\n",
    "    \n",
    "    resampled_idx = np.concatenate([res_pos_idx, neg_idx], axis=0)\n",
    "    resampled_val = np.concatenate([res_pos_val, neg_val], axis=0)\n",
    "    resampled_label = np.concatenate([res_pos_label, neg_label], axis=0)\n",
    "\n",
    "    order = np.arange(len(resampled_label))\n",
    "    np.random.shuffle(order)    \n",
    "    return [resampled_idx[order], resampled_val[order]], resampled_label[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train_input, res_train_label = oversample(train_input, train_label, bool_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(tf.keras.Model):\n",
    "    def __init__(self, cfg):\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.feature_size = cfg['feature_size']\n",
    "        self.field_size = cfg['field_size']\n",
    "        self.embed_size = cfg['embed_size']\n",
    "        self.deep_nn = cfg['deep_nn']\n",
    "        \n",
    "        self.dropout_fm = cfg['dropout_fm']\n",
    "        self.dropout_deep = cfg['dropout_deep']\n",
    "        \n",
    "        # fm        \n",
    "        self.feature_weight = tf.keras.layers.Embedding(cfg['feature_size'], 1)\n",
    "        self.feature_embed = tf.keras.layers.Embedding(cfg['feature_size'], cfg['embed_size'])\n",
    "\n",
    "        # dnn\n",
    "        for layer in range(len(cfg['deep_nn'])):\n",
    "            setattr(self, 'dense_' + str(layer), tf.keras.layers.Dense(self.deep_nn[layer]))\n",
    "            setattr(self, 'batchNorm_' + str(layer), tf.keras.layers.BatchNormalization())\n",
    "            setattr(self, 'activation_' + str(layer), tf.keras.layers.Activation('relu'))\n",
    "            setattr(self, 'dropout_' + str(layer), tf.keras.layers.Dropout(self.dropout_deep))\n",
    "            \n",
    "        self.fc = tf.keras.layers.Dense(1, activation='sigmoid', \n",
    "                                        bias_initializer=tf.keras.initializers.Constant(cfg['output_bias']))\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        # inputs = [feature_idx, feature_val]\n",
    "        reshaped_feature_val = tf.cast(tf.reshape(inputs[1], shape=[-1,self.field_size,1]), tf.float32)\n",
    "        # linear        \n",
    "        weights = self.feature_weight(inputs[0])\n",
    "        linear = tf.reduce_sum(tf.multiply(weights,reshaped_feature_val),2)\n",
    "        \n",
    "        # fm  \n",
    "        embeddings = self.feature_embed(inputs[0])\n",
    "        second_inner = tf.multiply(embeddings,reshaped_feature_val)\n",
    "        \n",
    "        summed_features_emb = tf.reduce_sum(second_inner,1)\n",
    "        summed_features_emb_square = tf.square(summed_features_emb)\n",
    "        \n",
    "        squared_features_emb = tf.square(second_inner)\n",
    "        squared_sum_features_emb = tf.reduce_sum(squared_features_emb,1)\n",
    "        \n",
    "        fm = 0.5 * tf.subtract(summed_features_emb_square,squared_sum_features_emb)\n",
    "        \n",
    "        # dnn\n",
    "        y_deep = tf.reshape(embeddings,shape=[-1,self.field_size * self.embed_size])\n",
    "        for layer in range(0, len(self.deep_nn)):\n",
    "            y_deep = getattr(self, 'dense_' + str(layer))(y_deep)\n",
    "            y_deep = getattr(self, 'batchNorm_' + str(layer))(y_deep, training=training)\n",
    "            y_deep = getattr(self, 'activation_' + str(layer))(y_deep)\n",
    "            y_deep = getattr(self, 'dropout_' + str(layer))(y_deep, training=training)\n",
    "            \n",
    "        # concat\n",
    "        concat = tf.concat([linear, fm, y_deep], axis=1)                                \n",
    "        out = self.fc(concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', verbose=1, patience=10, mode='max',restore_best_weights=True)\n",
    "metrics = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "           tf.keras.metrics.BinaryAccuracy(name='bin_acc'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall'),\n",
    "           tf.keras.metrics.AUC(name='auc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 733910 samples, validate on 95234 samples\n",
      "Epoch 1/200\n",
      "733910/733910 [==============================] - 14s 19us/sample - loss: 0.6891 - tp: 16206.0000 - fp: 6646.0000 - tn: 360309.0000 - fn: 350749.0000 - bin_acc: 0.5130 - precision: 0.7092 - recall: 0.0442 - auc: 0.5451 - val_loss: 0.7140 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 91853.0000 - val_fn: 3381.0000 - val_bin_acc: 0.9645 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.4980\n",
      "Epoch 2/200\n",
      "733910/733910 [==============================] - 9s 12us/sample - loss: 0.6777 - tp: 64345.0000 - fp: 22111.0000 - tn: 344844.0000 - fn: 302610.0000 - bin_acc: 0.5575 - precision: 0.7443 - recall: 0.1753 - auc: 0.5898 - val_loss: 0.7590 - val_tp: 0.0000e+00 - val_fp: 31.0000 - val_tn: 91822.0000 - val_fn: 3381.0000 - val_bin_acc: 0.9642 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5034\n",
      "Epoch 3/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6674 - tp: 96372.0000 - fp: 29264.0000 - tn: 337691.0000 - fn: 270583.0000 - bin_acc: 0.5914 - precision: 0.7671 - recall: 0.2626 - auc: 0.6203 - val_loss: 0.8887 - val_tp: 1022.0000 - val_fp: 27536.0000 - val_tn: 64317.0000 - val_fn: 2359.0000 - val_bin_acc: 0.6861 - val_precision: 0.0358 - val_recall: 0.3023 - val_auc: 0.5031\n",
      "Epoch 4/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6595 - tp: 115172.0000 - fp: 30937.0000 - tn: 336018.0000 - fn: 251783.0000 - bin_acc: 0.6148 - precision: 0.7883 - recall: 0.3139 - auc: 0.6403 - val_loss: 1.0096 - val_tp: 2064.0000 - val_fp: 55363.0000 - val_tn: 36490.0000 - val_fn: 1317.0000 - val_bin_acc: 0.4048 - val_precision: 0.0359 - val_recall: 0.6105 - val_auc: 0.5056\n",
      "Epoch 5/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6537 - tp: 128174.0000 - fp: 31873.0000 - tn: 335082.0000 - fn: 238781.0000 - bin_acc: 0.6312 - precision: 0.8009 - recall: 0.3493 - auc: 0.6565 - val_loss: 1.1155 - val_tp: 2655.0000 - val_fp: 71453.0000 - val_tn: 20400.0000 - val_fn: 726.0000 - val_bin_acc: 0.2421 - val_precision: 0.0358 - val_recall: 0.7853 - val_auc: 0.5018\n",
      "Epoch 6/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6490 - tp: 137383.0000 - fp: 31792.0000 - tn: 335163.0000 - fn: 229572.0000 - bin_acc: 0.6439 - precision: 0.8121 - recall: 0.3744 - auc: 0.6687 - val_loss: 0.7254 - val_tp: 157.0000 - val_fp: 4418.0000 - val_tn: 87435.0000 - val_fn: 3224.0000 - val_bin_acc: 0.9198 - val_precision: 0.0343 - val_recall: 0.0464 - val_auc: 0.4966\n",
      "Epoch 7/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6444 - tp: 146458.0000 - fp: 31664.0000 - tn: 335291.0000 - fn: 220497.0000 - bin_acc: 0.6564 - precision: 0.8222 - recall: 0.3991 - auc: 0.6798 - val_loss: 0.7914 - val_tp: 606.0000 - val_fp: 15203.0000 - val_tn: 76650.0000 - val_fn: 2775.0000 - val_bin_acc: 0.8112 - val_precision: 0.0383 - val_recall: 0.1792 - val_auc: 0.5035\n",
      "Epoch 8/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6407 - tp: 153362.0000 - fp: 31347.0000 - tn: 335608.0000 - fn: 213593.0000 - bin_acc: 0.6663 - precision: 0.8303 - recall: 0.4179 - auc: 0.6898 - val_loss: 0.6993 - val_tp: 37.0000 - val_fp: 773.0000 - val_tn: 91080.0000 - val_fn: 3344.0000 - val_bin_acc: 0.9568 - val_precision: 0.0457 - val_recall: 0.0109 - val_auc: 0.4984\n",
      "Epoch 9/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6375 - tp: 159093.0000 - fp: 31039.0000 - tn: 335916.0000 - fn: 207862.0000 - bin_acc: 0.6745 - precision: 0.8368 - recall: 0.4335 - auc: 0.6983 - val_loss: 0.7383 - val_tp: 252.0000 - val_fp: 6805.0000 - val_tn: 85048.0000 - val_fn: 3129.0000 - val_bin_acc: 0.8957 - val_precision: 0.0357 - val_recall: 0.0745 - val_auc: 0.5000\n",
      "Epoch 10/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6345 - tp: 165085.0000 - fp: 31075.0000 - tn: 335880.0000 - fn: 201870.0000 - bin_acc: 0.6826 - precision: 0.8416 - recall: 0.4499 - auc: 0.7065 - val_loss: 0.7189 - val_tp: 153.0000 - val_fp: 3773.0000 - val_tn: 88080.0000 - val_fn: 3228.0000 - val_bin_acc: 0.9265 - val_precision: 0.0390 - val_recall: 0.0453 - val_auc: 0.5017\n",
      "Epoch 11/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6319 - tp: 169640.0000 - fp: 30583.0000 - tn: 336372.0000 - fn: 197315.0000 - bin_acc: 0.6895 - precision: 0.8473 - recall: 0.4623 - auc: 0.7135 - val_loss: 0.7606 - val_tp: 376.0000 - val_fp: 10487.0000 - val_tn: 81366.0000 - val_fn: 3005.0000 - val_bin_acc: 0.8583 - val_precision: 0.0346 - val_recall: 0.1112 - val_auc: 0.5028\n",
      "Epoch 12/200\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6290 - tp: 174732.0000 - fp: 30492.0000 - tn: 336463.0000 - fn: 192223.0000 - bin_acc: 0.6965 - precision: 0.8514 - recall: 0.4762 - auc: 0.7209 - val_loss: 0.7143 - val_tp: 124.0000 - val_fp: 3091.0000 - val_tn: 88762.0000 - val_fn: 3257.0000 - val_bin_acc: 0.9333 - val_precision: 0.0386 - val_recall: 0.0367 - val_auc: 0.5041\n",
      "Epoch 13/200\n",
      "733910/733910 [==============================] - 9s 12us/sample - loss: 0.6268 - tp: 178343.0000 - fp: 30101.0000 - tn: 336854.0000 - fn: 188612.0000 - bin_acc: 0.7020 - precision: 0.8556 - recall: 0.4860 - auc: 0.7263 - val_loss: 0.7237 - val_tp: 172.0000 - val_fp: 4595.0000 - val_tn: 87258.0000 - val_fn: 3209.0000 - val_bin_acc: 0.9181 - val_precision: 0.0361 - val_recall: 0.0509 - val_auc: 0.5003\n",
      "Epoch 14/200\n",
      "730000/733910 [============================>.] - ETA: 0s - loss: 0.6248 - tp: 181192.0000 - fp: 29855.0000 - tn: 335159.0000 - fn: 183794.0000 - bin_acc: 0.7073 - precision: 0.8585 - recall: 0.4964 - auc: 0.7325Restoring model weights from the end of the best epoch.\n",
      "733910/733910 [==============================] - 8s 11us/sample - loss: 0.6248 - tp: 182179.0000 - fp: 30006.0000 - tn: 336949.0000 - fn: 184776.0000 - bin_acc: 0.7073 - precision: 0.8586 - recall: 0.4965 - auc: 0.7325 - val_loss: 0.7801 - val_tp: 498.0000 - val_fp: 13615.0000 - val_tn: 78238.0000 - val_fn: 2883.0000 - val_bin_acc: 0.8268 - val_precision: 0.0353 - val_recall: 0.1473 - val_auc: 0.4955\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6140092850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepFM(cfg)\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr=1e-3), \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "              metrics=metrics)\n",
    "model.fit(res_train_input, res_train_label, epochs=cfg['epoch'], batch_size=cfg['batch'], shuffle=True, \n",
    "          verbose=1, callbacks = [early_stopping], validation_data=(validate_input, validate_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('base_pramas.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os \n",
    "path = 'init_params'\n",
    "os.mkdir(path)\n",
    "model.save_weights(path+'/weights', overwrite=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = DeepFM(cfg)\n",
    "model.load_weights('init_params/weights')\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(lr=1e-3), \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "              metrics=metrics)\n",
    "model.fit(res_train_input, res_train_label, epochs=cfg['epoch'], batch_size=cfg['batch'], shuffle=True, \n",
    "          verbose=1, callbacks = [early_stopping], validation_data=(validate_input, validate_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deep_fm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  257       \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1212672   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  422       \n",
      "=================================================================\n",
      "Total params: 1,314,087\n",
      "Trainable params: 1,313,063\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wei/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "demodemo = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deep_fm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  257       \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1212672   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  422       \n",
      "=================================================================\n",
      "Total params: 1,314,087\n",
      "Trainable params: 1,313,063\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "demodemo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
